{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b049999-077d-4d48-a02d-94c842050121",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modeling with BART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10de6561-c13a-42b5-8568-a6f5b0f357d8",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31e859cc-6f92-45a8-b982-330ca5f285e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/w266/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "import inspect\n",
    "\n",
    "#let's make longer output readable without horizontal scrolling\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "\n",
    "import regex as re\n",
    "\n",
    "import os, re\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# These auto classes load the right type of tokenizer and model based on a model name\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18307bb8-b877-46e9-9677-730da3c02123",
   "metadata": {},
   "source": [
    "## Necessary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93328b1c-b213-490d-8bc0-60efeef7cf30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e882053a-a2bd-4ff1-88fb-18a91632831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate.load(\"chrf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9431bcd0-d5e9-4523-802c-02f2f58f7fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_args(func):\n",
    "    signature = inspect.signature(func)\n",
    "    return {\n",
    "        k: v.default\n",
    "        for k, v in signature.parameters.items()\n",
    "        if v.default is not inspect.Parameter.empty\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7102e2f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function is used to generate candidates and scores using a Huggingface model. \n",
    "\n",
    "# Default hyperparameters in this function are the same as those in the Huggingface models.  \n",
    "# https://huggingface.co/docs/transformers/main_classes/text_generation\n",
    "\n",
    "def bart_scores(mod, data, do_sample = False, num_beams = 1, top_k = 50, num_beam_groups = 1):\n",
    "\n",
    "    bart_r1 = []\n",
    "    bart_r2 = []\n",
    "    bart_rL = []\n",
    "    bart_rLs = []\n",
    "    bart_chrf = []\n",
    "    texts = []\n",
    "    references = []\n",
    "    candidates = []\n",
    "\n",
    "    for i in range(int(len(data['text']))):\n",
    "\n",
    "        #art = ' '.join(df['article'][i].split(' ')[:1024]) #truncated to first 1024 words, because that is all the model can handle\n",
    "\n",
    "        candidate = mod(data['text'][i], \n",
    "                               truncation = True, #truncated to first 1024 words, because that is all the model can handle\n",
    "                               max_length = 256, # same as pegasus\n",
    "                               min_length = 0, \n",
    "                               do_sample = do_sample,\n",
    "                               num_beams = num_beams, \n",
    "                               top_k = top_k,\n",
    "                               num_beam_groups = num_beam_groups,\n",
    "                                )[0]\n",
    "        candidate = [candidate['summary_text']]\n",
    "        #pprint(candidate[0], compact=True)\n",
    "\n",
    "        ref = [data['summary'][i]]\n",
    "\n",
    "        references.append(ref)\n",
    "        \n",
    "        candidates.append(candidate)\n",
    "        \n",
    "        texts.append(data['text'][i])\n",
    "        \n",
    "        results = rouge.compute(predictions=candidate,\n",
    "                                references=ref)\n",
    "\n",
    "        bart_r1.append(results['rouge1'])\n",
    "        bart_r2.append(results['rouge2'])\n",
    "        bart_rL.append(results['rougeL'])\n",
    "        bart_rLs.append(results['rougeLsum'])\n",
    "\n",
    "        results = chrf.compute(predictions=candidate,\n",
    "                                references=ref)\n",
    "\n",
    "        bart_chrf.append(results['score'])\n",
    "    \n",
    "    print('Last Article', data['text'][i])\n",
    "    print('Last Reference Summary', ref)\n",
    "    print('Last Candidate Summary', candidate)\n",
    "\n",
    "    print('rouge1 average :', np.mean(bart_r1))\n",
    "    print('rouge2 average :', np.mean(bart_r2))\n",
    "    print('rougeL average :', np.mean(bart_rL))\n",
    "    print('rougeLs average :', np.mean(bart_rLs))\n",
    "    print('chrf average :', np.mean(bart_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1078c92a-e541-4d8d-b768-729a7cad80cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Huggingface Transformers Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4aa535-8660-46a1-aee2-3b5cda0cb4af",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ff891-e53e-42e3-be54-e41fd95d590e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c449ad-39e8-44c5-9286-2d38106aefd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anthony ZurcherNorth America reporter@awzurche...</td>\n",
       "      <td>On day three of public hearings in the impeach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It made a net profit of $281m (£185m) in the t...</td>\n",
       "      <td>Yum Brands, owner of KFC and Pizza Hut restaur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Police sources told local media that the boy h...</td>\n",
       "      <td>Four members of the same family have been arre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zelda Perkins told the Financial Times she sig...</td>\n",
       "      <td>A British former assistant of Harvey Weinstein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bus workers walked out on Monday over changes ...</td>\n",
       "      <td>Bus drivers in Jersey have agreed to meet with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Anthony ZurcherNorth America reporter@awzurche...   \n",
       "1  It made a net profit of $281m (£185m) in the t...   \n",
       "2  Police sources told local media that the boy h...   \n",
       "3  Zelda Perkins told the Financial Times she sig...   \n",
       "4  Bus workers walked out on Monday over changes ...   \n",
       "\n",
       "                                             summary  \n",
       "0  On day three of public hearings in the impeach...  \n",
       "1  Yum Brands, owner of KFC and Pizza Hut restaur...  \n",
       "2  Four members of the same family have been arre...  \n",
       "3  A British former assistant of Harvey Weinstein...  \n",
       "4  Bus drivers in Jersey have agreed to meet with...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation Set\n",
    "df = pd.read_csv('../Data/xl_sum_sample_val.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e1260",
   "metadata": {},
   "source": [
    "## Model (Untrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f28d56-0e5f-4fae-8795-ef501326b2db",
   "metadata": {},
   "source": [
    "### Model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bf518e9-74f9-4b8a-813f-358e2dd958b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a8f757e-a407-41ac-9f2a-d5def79264a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/w266/lib/python3.8/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Your max_length is set to 256, but you input_length is only 172. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=86)\n",
      "Your max_length is set to 256, but you input_length is only 123. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      "Your max_length is set to 256, but you input_length is only 221. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
      "Your max_length is set to 256, but you input_length is only 255. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=127)\n",
      "Your max_length is set to 256, but you input_length is only 56. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 256, but you input_length is only 151. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=75)\n",
      "Your max_length is set to 256, but you input_length is only 220. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
      "Your max_length is set to 256, but you input_length is only 219. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=109)\n",
      "Your max_length is set to 256, but you input_length is only 191. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=95)\n",
      "Your max_length is set to 256, but you input_length is only 240. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=120)\n",
      "Your max_length is set to 256, but you input_length is only 212. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=106)\n",
      "Your max_length is set to 256, but you input_length is only 227. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=113)\n",
      "Your max_length is set to 256, but you input_length is only 80. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "Your max_length is set to 256, but you input_length is only 163. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=81)\n",
      "Your max_length is set to 256, but you input_length is only 202. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=101)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Article She tweeted that the clothes would be \"archived & expertly cared for in the spirit & love of Michael Jackson, his bravery, & fans worldwide\". The auction included a jacket worn during Jackson's Bad tour, that went for $240,000 (£148,000) and two crystal gloves. The items were all made by designers Dennis Tompkins and Michael Bush. Lady Gaga also tweeted a picture of herself and her bidding paddle at the auction. More than $5m (£3.1m) was raised by the sale, according to LA-based Julien's Auctions. Other items that went under the hammer included jackets from Michael Jackson's Dangerous and Thriller tours and a pair of jeans that went for $50,000 (£31,000). Some of the money raised by the auction is being donated to a guide dogs charity and a hospice in Las Vegas. American costume designers Michael Bush and Dennis Tompkins created thousands of original pieces for Michael Jackson during his career. However, despite Lady Gaga's assurances, some fans expressed their anger online, claiming that Jackson wanted his costumes to end up in a museum. The singer died on 25 June 2009 from an overdose of the powerful anaesthetic propofol. His personal doctor, Conrad Murray, was later found guilty of his involuntary manslaughter and sentenced to four years in jail. Follow @BBCNewsbeat on Twitter\n",
      "Last Reference Summary ['Lady Gaga has bought 55 items at an auction of Michael Jackson costumes in Los Angeles.']\n",
      "Last Candidate Summary ['She tweeted that the clothes would be \"archived & expertly cared for in the spirit & love of Michael Jackson, his bravery, & fans worldwide\". The auction included a jacket worn during Jackson\\'s Bad tour, that went for $240,000 (£148,000) and two crystal gloves. The items were all made by designers Dennis Tompkins and Michael Bush. Lady Gaga also tweeted a picture of herself and her bidding, which was taken at the auction. More than $5m (£3.1m) was raised by the sale, according to LA-based Julien\\'s Auctions. Other items that went under the hammer included jackets from Michael Jackson\\'s Dangerous and Thriller tours and a pair of jeans that went at $50,000 (31,000). Some of the money raised by a charity is being donated to a guide dogs charity and a hospice in Las Vegas. American costume designers Michael Bush and Dennis Tiddkins created thousands of original pieces for Michael Jackson during his career. However, despite Lady Gaga\\'s assurances, some fans expressed their anger online, claiming that Jackson wanted his costumes to end up in a museum. The singer died on 25 June 2009 from an overdose of the powerful anaesthetic propofol. His personal doctor']\n",
      "rouge1 average : 0.12452896925588756\n",
      "rouge2 average : 0.025831644973549065\n",
      "rougeL average : 0.07987155620518394\n",
      "rougeLs average : 0.07987155620518394\n",
      "chrf average : 19.959411116234886\n"
     ]
    }
   ],
   "source": [
    "bart_scores(summarizer, df, do_sample = False, num_beams = 1, top_k = 10, num_beam_groups = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d06867-89c4-40cf-a211-319f3d3a88f2",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66457876-a35d-43d3-84f4-57ce8cddefb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8595ddc4-4b0e-41d1-9afa-d3497a83a554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but you input_length is only 172. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=86)\n",
      "Your max_length is set to 256, but you input_length is only 123. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      "Your max_length is set to 256, but you input_length is only 221. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
      "Your max_length is set to 256, but you input_length is only 255. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=127)\n",
      "Your max_length is set to 256, but you input_length is only 56. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 256, but you input_length is only 151. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=75)\n",
      "Your max_length is set to 256, but you input_length is only 220. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
      "Your max_length is set to 256, but you input_length is only 219. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=109)\n",
      "Your max_length is set to 256, but you input_length is only 191. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=95)\n",
      "Your max_length is set to 256, but you input_length is only 240. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=120)\n",
      "Your max_length is set to 256, but you input_length is only 212. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=106)\n",
      "Your max_length is set to 256, but you input_length is only 227. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=113)\n",
      "Your max_length is set to 256, but you input_length is only 80. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "Your max_length is set to 256, but you input_length is only 163. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=81)\n",
      "Your max_length is set to 256, but you input_length is only 202. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=101)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Article She tweeted that the clothes would be \"archived & expertly cared for in the spirit & love of Michael Jackson, his bravery, & fans worldwide\". The auction included a jacket worn during Jackson's Bad tour, that went for $240,000 (£148,000) and two crystal gloves. The items were all made by designers Dennis Tompkins and Michael Bush. Lady Gaga also tweeted a picture of herself and her bidding paddle at the auction. More than $5m (£3.1m) was raised by the sale, according to LA-based Julien's Auctions. Other items that went under the hammer included jackets from Michael Jackson's Dangerous and Thriller tours and a pair of jeans that went for $50,000 (£31,000). Some of the money raised by the auction is being donated to a guide dogs charity and a hospice in Las Vegas. American costume designers Michael Bush and Dennis Tompkins created thousands of original pieces for Michael Jackson during his career. However, despite Lady Gaga's assurances, some fans expressed their anger online, claiming that Jackson wanted his costumes to end up in a museum. The singer died on 25 June 2009 from an overdose of the powerful anaesthetic propofol. His personal doctor, Conrad Murray, was later found guilty of his involuntary manslaughter and sentenced to four years in jail. Follow @BBCNewsbeat on Twitter\n",
      "Last Reference Summary ['Lady Gaga has bought 55 items at an auction of Michael Jackson costumes in Los Angeles.']\n",
      "Last Candidate Summary ['She tweeted that the clothes would be \"archived & expertly cared for in the spirit & love of Michael Jackson, his bravery, & fans worldwide\". The auction included a jacket worn during Jackson\\'s Bad tour, that went for $240,000 (£148,000) and two crystal gloves. The items were all made by designers Dennis Tompkins and Michael Bush. Lady Gaga also tweeted a picture of herself and her bidding for the items, which she said would be auctioned off at the auction. More than $5m (£3.1m) was raised by the sale, according to LA-based Julien\\'s Auctions. Other items that went under the hammer included jackets from Michael Jackson\\'s Dangerous and Thriller tours and a pair of jeans that went on sale for $50,000 (31,000). Some of the money collected by the auction is being donated to a guide dogs charity and a hospice in Las Vegas. American costume designers Michael Bush and Dennis Tommkins created thousands of original pieces for Michael Jackson during his career. However, despite Lady Gaga\\'s assurances, some fans expressed their anger online, claiming that Jackson wanted his costumes to end up in a museum. The singer died on 25 June 2009 from an overdose of the']\n",
      "rouge1 average : 0.12455263030601277\n",
      "rouge2 average : 0.025373409566807396\n",
      "rougeL average : 0.08078374063663701\n",
      "rougeLs average : 0.08078374063663701\n",
      "chrf average : 19.944723571668213\n"
     ]
    }
   ],
   "source": [
    "bart_scores(summarizer, df, do_sample = True, num_beams = 2, top_k = 10, num_beam_groups = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f7b2e3-3e42-4078-b6a1-69ef817b5ff0",
   "metadata": {},
   "source": [
    "### Model 2 **BEST MODEL TIED WITH MODEL 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53bea3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a99d01f-043b-4145-a57d-c470bde70979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but you input_length is only 172. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=86)\n",
      "Your max_length is set to 256, but you input_length is only 123. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      "Your max_length is set to 256, but you input_length is only 221. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
      "Your max_length is set to 256, but you input_length is only 255. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=127)\n",
      "Your max_length is set to 256, but you input_length is only 56. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 256, but you input_length is only 151. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=75)\n",
      "Your max_length is set to 256, but you input_length is only 220. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
      "Your max_length is set to 256, but you input_length is only 219. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=109)\n",
      "Your max_length is set to 256, but you input_length is only 191. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=95)\n",
      "Your max_length is set to 256, but you input_length is only 240. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=120)\n",
      "Your max_length is set to 256, but you input_length is only 212. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=106)\n",
      "Your max_length is set to 256, but you input_length is only 227. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=113)\n",
      "Your max_length is set to 256, but you input_length is only 80. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "Your max_length is set to 256, but you input_length is only 163. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=81)\n",
      "Your max_length is set to 256, but you input_length is only 202. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=101)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Article She tweeted that the clothes would be \"archived & expertly cared for in the spirit & love of Michael Jackson, his bravery, & fans worldwide\". The auction included a jacket worn during Jackson's Bad tour, that went for $240,000 (£148,000) and two crystal gloves. The items were all made by designers Dennis Tompkins and Michael Bush. Lady Gaga also tweeted a picture of herself and her bidding paddle at the auction. More than $5m (£3.1m) was raised by the sale, according to LA-based Julien's Auctions. Other items that went under the hammer included jackets from Michael Jackson's Dangerous and Thriller tours and a pair of jeans that went for $50,000 (£31,000). Some of the money raised by the auction is being donated to a guide dogs charity and a hospice in Las Vegas. American costume designers Michael Bush and Dennis Tompkins created thousands of original pieces for Michael Jackson during his career. However, despite Lady Gaga's assurances, some fans expressed their anger online, claiming that Jackson wanted his costumes to end up in a museum. The singer died on 25 June 2009 from an overdose of the powerful anaesthetic propofol. His personal doctor, Conrad Murray, was later found guilty of his involuntary manslaughter and sentenced to four years in jail. Follow @BBCNewsbeat on Twitter\n",
      "Last Reference Summary ['Lady Gaga has bought 55 items at an auction of Michael Jackson costumes in Los Angeles.']\n",
      "Last Candidate Summary ['She tweeted that the clothes would be \"archived & expertly cared for in the spirit & love of Michael Jackson, his bravery, & fans worldwide\". The auction included a jacket worn during Jackson\\'s Bad tour, that went for $240,000 (£148,000) and two crystal gloves. The items were all made by designers Dennis Tompkins and Michael Bush. Lady Gaga also tweeted a picture of herself and her bidding, which was taken at the auction. More than $5m (£3.1m) was raised by the sale, according to LA-based Julien\\'s Auctions. Other items that went under the hammer included jackets from Michael Jackson\\'s Dangerous and Thriller tours and a pair of jeans that went at $50,000 (31,000). Some of the money raised by a charity is being donated to a guide dogs charity and a hospice in Las Vegas. American costume designers Michael Bush and Dennis Tiddkins created thousands of original pieces for Michael Jackson during his career. However, despite Lady Gaga\\'s assurances, some fans expressed their anger online, claiming that Jackson wanted his costumes to end up in a museum. The singer died on 25 June 2009 from an overdose of the powerful anaesthetic propofol. His personal doctor']\n",
      "rouge1 average : 0.12452896925588756\n",
      "rouge2 average : 0.025831644973549065\n",
      "rougeL average : 0.07987155620518394\n",
      "rougeLs average : 0.07987155620518394\n",
      "chrf average : 19.959411116234886\n"
     ]
    }
   ],
   "source": [
    "bart_scores(summarizer, df, do_sample = False, num_beams = 1, top_k = 50, num_beam_groups = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3168e22",
   "metadata": {},
   "source": [
    "## Model (Trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdae58a-e4c0-4a6c-8f67-b8309a24c665",
   "metadata": {},
   "source": [
    "### Model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41489617-caa3-421b-861a-7716cb853985",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 transformers/examples/pytorch/summarization/run_summarization.py \\\n",
    "    --model_name_or_path=facebook/bart-base \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_file 'w266_project/xl_sum_sample_train.csv' \\\n",
    "    --validation_file 'w266_project/xl_sum_sample_val.csv' \\\n",
    "    --text_column text \\\n",
    "    --summary_column summary \\\n",
    "    --push_to_hub=True \\\n",
    "    --max_source_length 128 \\\n",
    "    --max_target_length 32 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size=32 \\\n",
    "    --per_device_eval_batch_size=32 \\\n",
    "    --output_dir='w266_project/finetuned-BART-all-categories/model 0/finetuned-BART-all-categories' \\\n",
    "    --overwrite_output_dir=True \\\n",
    "    --predict_with_generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7a1f378c-07b9-48ec-b77c-c4bc558143af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"arisanguyen/finetuned-BART-all-categories\", revision = 'model_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c26b8a9c-6385-4f29-b3fc-caa2bbbd1c10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/w266/lib/python3.8/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Your max_length is set to 128, but you input_length is only 123. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      "Your max_length is set to 128, but you input_length is only 56. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 128, but you input_length is only 80. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n"
     ]
    }
   ],
   "source": [
    "bart_r1 = []\n",
    "bart_r2 = []\n",
    "bart_rL = []\n",
    "bart_rLs = []\n",
    "bart_chrf = []\n",
    "\n",
    "for i in range(int(len(df['text']))):\n",
    "    \n",
    "    candidate = summarizer(df['text'][i], \n",
    "                           truncation = True, #truncated to first 1024 words, because that is all the model can handle\n",
    "                            )[0]\n",
    "    candidate = [candidate['summary_text']]\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bart_r1.append(results['rouge1'])\n",
    "    bart_r2.append(results['rouge2'])\n",
    "    bart_rL.append(results['rougeL'])\n",
    "    bart_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    results = chrf.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bart_chrf.append(results['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5ed12cd2-800e-43b2-b5d3-445b531d7326",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Article She tweeted that the clothes would be \"archived & expertly cared for in the spirit & love of Michael Jackson, his bravery, & fans worldwide\". The auction included a jacket worn during Jackson's Bad tour, that went for $240,000 (£148,000) and two crystal gloves. The items were all made by designers Dennis Tompkins and Michael Bush. Lady Gaga also tweeted a picture of herself and her bidding paddle at the auction. More than $5m (£3.1m) was raised by the sale, according to LA-based Julien's Auctions. Other items that went under the hammer included jackets from Michael Jackson's Dangerous and Thriller tours and a pair of jeans that went for $50,000 (£31,000). Some of the money raised by the auction is being donated to a guide dogs charity and a hospice in Las Vegas. American costume designers Michael Bush and Dennis Tompkins created thousands of original pieces for Michael Jackson during his career. However, despite Lady Gaga's assurances, some fans expressed their anger online, claiming that Jackson wanted his costumes to end up in a museum. The singer died on 25 June 2009 from an overdose of the powerful anaesthetic propofol. His personal doctor, Conrad Murray, was later found guilty of his involuntary manslaughter and sentenced to four years in jail. Follow @BBCNewsbeat on Twitter\n",
      "Last Reference Summary ['Lady Gaga has bought 55 items at an auction of Michael Jackson costumes in Los Angeles.']\n",
      "Last Candidate Summary [\"Lady Gaga has announced that she will be auctioning off a collection of Michael Jackson's costumes at an auction in Las Vegas.\"]\n"
     ]
    }
   ],
   "source": [
    "print('Last Article', df['text'][i])\n",
    "print('Last Reference Summary', ref)\n",
    "print('Last Candidate Summary', candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9ae281a4-5d2e-48fa-a4a4-0a500d69e539",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.3192639042917483\n",
      "rouge2 average : 0.10466960084268205\n",
      "rougeL average : 0.24501046685959502\n",
      "rougeLs average : 0.24501046685959502\n",
      "chrf average : 27.774860738085867\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bart_r1))\n",
    "print('rouge2 average :', np.mean(bart_r2))\n",
    "print('rougeL average :', np.mean(bart_rL))\n",
    "print('rougeLs average :', np.mean(bart_rLs))\n",
    "print('chrf average :', np.mean(bart_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d130492-21d3-4af9-b626-9fa36e271252",
   "metadata": {},
   "source": [
    "### Model 1 (number of epochs to 62 (~2k steps total) to be same as PEGASUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9182eab-2816-4040-82cb-9b85b74ceded",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 transformers/examples/pytorch/summarization/run_summarization.py \\\n",
    "    --model_name_or_path=facebook/bart-base \\\n",
    "    --do_train \\\n",
    "    --train_file 'w266_project/xl_sum_sample_train.csv' \\\n",
    "    --text_column text \\\n",
    "    --summary_column summary \\\n",
    "    --max_source_length 512 \\\n",
    "    --max_target_length 256 \\\n",
    "    --num_train_epochs 62 \\\n",
    "    --per_device_train_batch_size=32 \\\n",
    "    --push_to_hub=True \\\n",
    "    --output_dir='w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories' \\\n",
    "    --overwrite_output_dir=True \\\n",
    "    --predict_with_generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "06c6c9a2-07fa-48ac-bb37-2395345a49a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"arisanguyen/finetuned-BART-all-categories\", revision = \"model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9062b049-9393-44a7-86ec-c5d0cdacdba9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/w266/lib/python3.8/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but you input_length is only 172. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=86)\n",
      "Your max_length is set to 256, but you input_length is only 123. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      "Your max_length is set to 256, but you input_length is only 221. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
      "Your max_length is set to 256, but you input_length is only 255. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=127)\n",
      "Your max_length is set to 256, but you input_length is only 56. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 256, but you input_length is only 151. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=75)\n",
      "Your max_length is set to 256, but you input_length is only 220. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
      "Your max_length is set to 256, but you input_length is only 219. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=109)\n",
      "Your max_length is set to 256, but you input_length is only 191. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=95)\n",
      "Your max_length is set to 256, but you input_length is only 240. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=120)\n",
      "Your max_length is set to 256, but you input_length is only 212. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=106)\n",
      "Your max_length is set to 256, but you input_length is only 227. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=113)\n",
      "Your max_length is set to 256, but you input_length is only 80. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "Your max_length is set to 256, but you input_length is only 163. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=81)\n",
      "Your max_length is set to 256, but you input_length is only 202. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=101)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Article She tweeted that the clothes would be \"archived & expertly cared for in the spirit & love of Michael Jackson, his bravery, & fans worldwide\". The auction included a jacket worn during Jackson's Bad tour, that went for $240,000 (£148,000) and two crystal gloves. The items were all made by designers Dennis Tompkins and Michael Bush. Lady Gaga also tweeted a picture of herself and her bidding paddle at the auction. More than $5m (£3.1m) was raised by the sale, according to LA-based Julien's Auctions. Other items that went under the hammer included jackets from Michael Jackson's Dangerous and Thriller tours and a pair of jeans that went for $50,000 (£31,000). Some of the money raised by the auction is being donated to a guide dogs charity and a hospice in Las Vegas. American costume designers Michael Bush and Dennis Tompkins created thousands of original pieces for Michael Jackson during his career. However, despite Lady Gaga's assurances, some fans expressed their anger online, claiming that Jackson wanted his costumes to end up in a museum. The singer died on 25 June 2009 from an overdose of the powerful anaesthetic propofol. His personal doctor, Conrad Murray, was later found guilty of his involuntary manslaughter and sentenced to four years in jail. Follow @BBCNewsbeat on Twitter\n",
      "Last Reference Summary ['Lady Gaga has bought 55 items at an auction of Michael Jackson costumes in Los Angeles.']\n",
      "Last Candidate Summary ['Lady Gaga has auctioned off hundreds of items worth more than $190,000 (£152,000) at an auction in Las Vegas.']\n",
      "rouge1 average : 0.31336770582535894\n",
      "rouge2 average : 0.09975946851022606\n",
      "rougeL average : 0.23881053257054677\n",
      "rougeLs average : 0.23881053257054677\n",
      "chrf average : 28.86864045130219\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./w266_project/Datasets/xl_sum_sample_val.csv')\n",
    "\n",
    "bart_r1 = []\n",
    "bart_r2 = []\n",
    "bart_rL = []\n",
    "bart_rLs = []\n",
    "bart_chrf = []\n",
    "\n",
    "for i in range(int(len(df['text']))):\n",
    "    \n",
    "    candidate = summarizer(df['text'][i], \n",
    "                           truncation = True, #truncated to first 1024 words, because that is all the model can handle\n",
    "                           max_length = 256, # same as pegasus\n",
    "                           min_length = 0, \n",
    "                             #max_length=130, min_length=30, do_sample=False\n",
    "                            )[0]\n",
    "    candidate = [candidate['summary_text']]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bart_r1.append(results['rouge1'])\n",
    "    bart_r2.append(results['rouge2'])\n",
    "    bart_rL.append(results['rougeL'])\n",
    "    bart_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    results = chrf.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bart_chrf.append(results['score'])\n",
    "    \n",
    "    if i in np.arange(0, 2000, 100):\n",
    "        data = {'rouge1': bart_r1, 'rouge2': bart_r2, 'rogueL': bart_rL, 'rogueLs': bart_rLs, 'chrf': bart_chrf}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BART_trained_1_scores.csv', index=False)\n",
    "        print(i)\n",
    "        \n",
    "data = {'rouge1': bart_r1, 'rouge2': bart_r2, 'rogueL': bart_rL, 'rogueLs': bart_rLs,'chrf': bart_chrf}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BART_trained_1_scores.csv', index=False)\n",
    "\n",
    "print('Last Article', df['text'][i])\n",
    "print('Last Reference Summary', ref)\n",
    "print('Last Candidate Summary', candidate)\n",
    "\n",
    "print('rouge1 average :', np.mean(bart_r1))\n",
    "print('rouge2 average :', np.mean(bart_r2))\n",
    "print('rougeL average :', np.mean(bart_rL))\n",
    "print('rougeLs average :', np.mean(bart_rLs))\n",
    "print('chrf average :', np.mean(bart_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5276d8de",
   "metadata": {},
   "source": [
    "### Model 2 (added beams, chagned epochs, added validation target length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bffb3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/ubuntu/w266/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "04/07/2023 15:57:38 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "04/07/2023 15:57:38 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories/runs/Apr07_15-57-38_ip-172-31-55-240,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "04/07/2023 15:57:38 - INFO - datasets.builder - Using custom data configuration default-6794da18f7676227\n",
      "04/07/2023 15:57:38 - INFO - datasets.info - Loading Dataset Infos from /home/ubuntu/w266/lib/python3.8/site-packages/datasets/packaged_modules/csv\n",
      "04/07/2023 15:57:38 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "04/07/2023 15:57:38 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/csv/default-6794da18f7676227/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
      "04/07/2023 15:57:38 - WARNING - datasets.builder - Found cached dataset csv (/home/ubuntu/.cache/huggingface/datasets/csv/default-6794da18f7676227/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "04/07/2023 15:57:38 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/csv/default-6794da18f7676227/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 684.78it/s]\n",
      "[INFO|configuration_utils.py:668] 2023-04-07 15:57:38,934 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-04-07 15:57:38,941 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:494] 2023-04-07 15:57:38,973 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:668] 2023-04-07 15:57:39,000 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-04-07 15:57:39,000 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-07 15:57:39,070 >> loading file vocab.json from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-07 15:57:39,070 >> loading file merges.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-07 15:57:39,070 >> loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-07 15:57:39,070 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-07 15:57:39,070 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-07 15:57:39,070 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:668] 2023-04-07 15:57:39,070 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-04-07 15:57:39,071 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2451] 2023-04-07 15:57:39,172 >> loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:575] 2023-04-07 15:57:42,695 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.28.0.dev0\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3100] 2023-04-07 15:57:45,076 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3108] 2023-04-07 15:57:45,076 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "[INFO|modeling_utils.py:2753] 2023-04-07 15:57:45,114 >> Generation config file not found, using a generation config created from the model config.\n",
      "04/07/2023 15:57:45 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/csv/default-6794da18f7676227/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6f5dbeec5a6410bc.arrow\n",
      "/home/ubuntu/w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories is already a clone of https://huggingface.co/arisanguyen/finetuned-BART-all-categories. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "04/07/2023 15:57:45 - WARNING - huggingface_hub.repository - /home/ubuntu/w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories is already a clone of https://huggingface.co/arisanguyen/finetuned-BART-all-categories. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "/home/ubuntu/w266/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1746] 2023-04-07 15:57:47,848 >> ***** Running training *****\n",
      "[INFO|trainer.py:1747] 2023-04-07 15:57:47,848 >>   Num examples = 1000\n",
      "[INFO|trainer.py:1748] 2023-04-07 15:57:47,848 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1749] 2023-04-07 15:57:47,848 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1750] 2023-04-07 15:57:47,848 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1751] 2023-04-07 15:57:47,848 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1752] 2023-04-07 15:57:47,849 >>   Total optimization steps = 160\n",
      "[INFO|trainer.py:1753] 2023-04-07 15:57:47,849 >>   Number of trainable parameters = 139420416\n",
      "  0%|                                                   | 0/160 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-04-07 15:57:47,875 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|█████████████████████████████████████████| 160/160 [44:03<00:00, 13.19s/it][INFO|trainer.py:2016] 2023-04-07 16:41:50,888 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2643.0389, 'train_samples_per_second': 1.892, 'train_steps_per_second': 0.061, 'train_loss': 2.1022010803222657, 'epoch': 5.0}\n",
      "100%|█████████████████████████████████████████| 160/160 [44:03<00:00, 16.52s/it]\n",
      "[INFO|trainer.py:2821] 2023-04-07 16:41:50,890 >> Saving model checkpoint to w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories\n",
      "[INFO|configuration_utils.py:457] 2023-04-07 16:41:50,891 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-07 16:41:50,892 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories/generation_config.json\n",
      "[INFO|modeling_utils.py:1812] 2023-04-07 16:41:54,602 >> Model weights saved in w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-07 16:41:54,603 >> tokenizer config file saved in w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2177] 2023-04-07 16:41:54,603 >> Special tokens file saved in w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories/special_tokens_map.json\n",
      "[INFO|trainer.py:2821] 2023-04-07 16:41:54,702 >> Saving model checkpoint to w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories\n",
      "[INFO|configuration_utils.py:457] 2023-04-07 16:41:54,703 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-07 16:41:54,704 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories/generation_config.json\n",
      "[INFO|modeling_utils.py:1812] 2023-04-07 16:41:58,804 >> Model weights saved in w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-07 16:41:58,805 >> tokenizer config file saved in w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2177] 2023-04-07 16:41:58,805 >> Special tokens file saved in w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories/special_tokens_map.json\n",
      "To https://huggingface.co/arisanguyen/finetuned-BART-all-categories\n",
      "   910e579..35a942e  main -> main\n",
      "\n",
      "04/07/2023 16:42:05 - WARNING - huggingface_hub.repository - To https://huggingface.co/arisanguyen/finetuned-BART-all-categories\n",
      "   910e579..35a942e  main -> main\n",
      "\n",
      "[INFO|modelcard.py:451] 2023-04-07 16:42:08,413 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}}\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  train_loss               =     2.1022\n",
      "  train_runtime            = 0:44:03.03\n",
      "  train_samples            =       1000\n",
      "  train_samples_per_second =      1.892\n",
      "  train_steps_per_second   =      0.061\n",
      "[INFO|trainer.py:2821] 2023-04-07 16:42:08,566 >> Saving model checkpoint to w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories\n",
      "[INFO|configuration_utils.py:457] 2023-04-07 16:42:08,567 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-07 16:42:08,569 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories/generation_config.json\n",
      "[INFO|modeling_utils.py:1812] 2023-04-07 16:42:12,367 >> Model weights saved in w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-07 16:42:12,367 >> tokenizer config file saved in w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2177] 2023-04-07 16:42:12,368 >> Special tokens file saved in w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories/special_tokens_map.json\n",
      "To https://huggingface.co/arisanguyen/finetuned-BART-all-categories\n",
      "   35a942e..f71b33c  main -> main\n",
      "\n",
      "04/07/2023 16:42:18 - WARNING - huggingface_hub.repository - To https://huggingface.co/arisanguyen/finetuned-BART-all-categories\n",
      "   35a942e..f71b33c  main -> main\n",
      "\n",
      "[INFO|modelcard.py:451] 2023-04-07 16:42:22,289 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Summarization', 'type': 'summarization'}}\n"
     ]
    }
   ],
   "source": [
    "!python3 transformers/examples/pytorch/summarization/run_summarization.py \\\n",
    "    --model_name_or_path=facebook/bart-base \\\n",
    "    --do_train \\\n",
    "    --train_file 'w266_project/Datasets/xl_sum_sample_train.csv' \\\n",
    "    --text_column text \\\n",
    "    --summary_column summary \\\n",
    "    --max_source_length 512 \\\n",
    "    --max_target_length 256 \\\n",
    "    --val_max_target_length 256 \\\n",
    "    --num_beams 5 \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --per_device_train_batch_size=32 \\\n",
    "    --push_to_hub=True \\\n",
    "    --output_dir='w266_project/finetuned-BART-all-categories/model 2/finetuned-BART-all-categories' \\\n",
    "    --overwrite_output_dir=True \\\n",
    "    --predict_with_generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3043b773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"arisanguyen/finetuned-BART-all-categories\", revision = \"model_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9bd6a2f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but you input_length is only 172. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=86)\n",
      "Your max_length is set to 256, but you input_length is only 123. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      "Your max_length is set to 256, but you input_length is only 221. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
      "Your max_length is set to 256, but you input_length is only 255. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=127)\n",
      "Your max_length is set to 256, but you input_length is only 56. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 256, but you input_length is only 151. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=75)\n",
      "Your max_length is set to 256, but you input_length is only 220. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
      "Your max_length is set to 256, but you input_length is only 219. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=109)\n",
      "Your max_length is set to 256, but you input_length is only 191. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=95)\n",
      "Your max_length is set to 256, but you input_length is only 240. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=120)\n",
      "Your max_length is set to 256, but you input_length is only 212. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=106)\n",
      "Your max_length is set to 256, but you input_length is only 227. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=113)\n",
      "Your max_length is set to 256, but you input_length is only 80. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "Your max_length is set to 256, but you input_length is only 163. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=81)\n",
      "Your max_length is set to 256, but you input_length is only 202. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=101)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Article She tweeted that the clothes would be \"archived & expertly cared for in the spirit & love of Michael Jackson, his bravery, & fans worldwide\". The auction included a jacket worn during Jackson's Bad tour, that went for $240,000 (£148,000) and two crystal gloves. The items were all made by designers Dennis Tompkins and Michael Bush. Lady Gaga also tweeted a picture of herself and her bidding paddle at the auction. More than $5m (£3.1m) was raised by the sale, according to LA-based Julien's Auctions. Other items that went under the hammer included jackets from Michael Jackson's Dangerous and Thriller tours and a pair of jeans that went for $50,000 (£31,000). Some of the money raised by the auction is being donated to a guide dogs charity and a hospice in Las Vegas. American costume designers Michael Bush and Dennis Tompkins created thousands of original pieces for Michael Jackson during his career. However, despite Lady Gaga's assurances, some fans expressed their anger online, claiming that Jackson wanted his costumes to end up in a museum. The singer died on 25 June 2009 from an overdose of the powerful anaesthetic propofol. His personal doctor, Conrad Murray, was later found guilty of his involuntary manslaughter and sentenced to four years in jail. Follow @BBCNewsbeat on Twitter\n",
      "Last Reference Summary ['Lady Gaga has bought 55 items at an auction of Michael Jackson costumes in Los Angeles.']\n",
      "Last Candidate Summary ['Lady Gaga has sold more than 100 pieces of clothing at an auction in Las Vegas.']\n",
      "rouge1 average : 0.32314773160661303\n",
      "rouge2 average : 0.10284455778759283\n",
      "rougeL average : 0.24857874693267637\n",
      "rougeLs average : 0.24857874693267637\n",
      "chrf average : 28.11065337271046\n"
     ]
    }
   ],
   "source": [
    "bart_scores(summarizer, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799676d",
   "metadata": {},
   "source": [
    "### Model 3 (best training parameters above with new task specific paramters) **BEST MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f4219e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"arisanguyen/finetuned-BART-all-categories\", revision = \"model_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0bd3c231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but you input_length is only 172. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=86)\n",
      "Your max_length is set to 256, but you input_length is only 123. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      "Your max_length is set to 256, but you input_length is only 221. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
      "Your max_length is set to 256, but you input_length is only 255. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=127)\n",
      "Your max_length is set to 256, but you input_length is only 56. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 256, but you input_length is only 151. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=75)\n",
      "Your max_length is set to 256, but you input_length is only 220. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
      "Your max_length is set to 256, but you input_length is only 219. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=109)\n",
      "Your max_length is set to 256, but you input_length is only 191. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=95)\n",
      "Your max_length is set to 256, but you input_length is only 240. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=120)\n",
      "Your max_length is set to 256, but you input_length is only 212. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=106)\n",
      "Your max_length is set to 256, but you input_length is only 227. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=113)\n",
      "Your max_length is set to 256, but you input_length is only 80. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "Your max_length is set to 256, but you input_length is only 163. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=81)\n",
      "Your max_length is set to 256, but you input_length is only 202. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=101)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Article She tweeted that the clothes would be \"archived & expertly cared for in the spirit & love of Michael Jackson, his bravery, & fans worldwide\". The auction included a jacket worn during Jackson's Bad tour, that went for $240,000 (£148,000) and two crystal gloves. The items were all made by designers Dennis Tompkins and Michael Bush. Lady Gaga also tweeted a picture of herself and her bidding paddle at the auction. More than $5m (£3.1m) was raised by the sale, according to LA-based Julien's Auctions. Other items that went under the hammer included jackets from Michael Jackson's Dangerous and Thriller tours and a pair of jeans that went for $50,000 (£31,000). Some of the money raised by the auction is being donated to a guide dogs charity and a hospice in Las Vegas. American costume designers Michael Bush and Dennis Tompkins created thousands of original pieces for Michael Jackson during his career. However, despite Lady Gaga's assurances, some fans expressed their anger online, claiming that Jackson wanted his costumes to end up in a museum. The singer died on 25 June 2009 from an overdose of the powerful anaesthetic propofol. His personal doctor, Conrad Murray, was later found guilty of his involuntary manslaughter and sentenced to four years in jail. Follow @BBCNewsbeat on Twitter\n",
      "Last Reference Summary ['Lady Gaga has bought 55 items at an auction of Michael Jackson costumes in Los Angeles.']\n",
      "Last Candidate Summary ['Lady Gaga has sold more than 100 pieces of clothing at an auction in Las Vegas.']\n",
      "rouge1 average : 0.3376935100553818\n",
      "rouge2 average : 0.11338485717992809\n",
      "rougeL average : 0.26158516350143535\n",
      "rougeLs average : 0.26158516350143535\n",
      "chrf average : 29.11464182430708\n"
     ]
    }
   ],
   "source": [
    "bart_scores(summarizer, df, do_sample = True, num_beams = 4, top_k = 75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
