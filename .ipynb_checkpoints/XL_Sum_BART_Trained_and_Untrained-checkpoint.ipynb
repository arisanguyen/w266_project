{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b049999-077d-4d48-a02d-94c842050121",
   "metadata": {
    "tags": []
   },
   "source": [
    "**BART Trained on XL SUM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d1de3e-85a6-4307-8abf-fbfcae0be263",
   "metadata": {},
   "source": [
    "**Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dfe809d-a7a8-47b6-a2e0-2d7bf7d74fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/w266/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "import inspect\n",
    "\n",
    "#let's make longer output readable without horizontal scrolling\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "\n",
    "import regex as re\n",
    "\n",
    "import os, re\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# These auto classes load the right type of tokenizer and model based on a model name\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ef6ab7-aa59-479f-a0e8-e0e34a21dc7f",
   "metadata": {},
   "source": [
    "**Necessary Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3330d3-069e-48dc-8a0b-e2ef7d72a2bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "216551a6-cb1e-4516-aa9b-b18d88807a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate.load(\"chrf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a32aaf5-c5bb-4052-8b50-af66dbb96312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_args(func):\n",
    "    signature = inspect.signature(func)\n",
    "    return {\n",
    "        k: v.default\n",
    "        for k, v in signature.parameters.items()\n",
    "        if v.default is not inspect.Parameter.empty\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44de8cc2-471c-4ad3-b94d-8473c6c3c244",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Training the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b616e-4e2b-46eb-83b7-75930d6a43ee",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py\n",
    "\n",
    "https://www.databricks.com/blog/2023/03/20/fine-tuning-large-language-models-hugging-face-and-deepspeed.html\n",
    "\n",
    "https://gitlab.com/nicolalandro/summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6399dc-0acd-49fc-a5f7-4c7d4e689781",
   "metadata": {},
   "source": [
    "**All Categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b36a5220",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anthony ZurcherNorth America reporter@awzurche...</td>\n",
       "      <td>On day three of public hearings in the impeach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It made a net profit of $281m (£185m) in the t...</td>\n",
       "      <td>Yum Brands, owner of KFC and Pizza Hut restaur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Police sources told local media that the boy h...</td>\n",
       "      <td>Four members of the same family have been arre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zelda Perkins told the Financial Times she sig...</td>\n",
       "      <td>A British former assistant of Harvey Weinstein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bus workers walked out on Monday over changes ...</td>\n",
       "      <td>Bus drivers in Jersey have agreed to meet with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Anthony ZurcherNorth America reporter@awzurche...   \n",
       "1  It made a net profit of $281m (£185m) in the t...   \n",
       "2  Police sources told local media that the boy h...   \n",
       "3  Zelda Perkins told the Financial Times she sig...   \n",
       "4  Bus workers walked out on Monday over changes ...   \n",
       "\n",
       "                                             summary  \n",
       "0  On day three of public hearings in the impeach...  \n",
       "1  Yum Brands, owner of KFC and Pizza Hut restaur...  \n",
       "2  Four members of the same family have been arre...  \n",
       "3  A British former assistant of Harvey Weinstein...  \n",
       "4  Bus drivers in Jersey have agreed to meet with...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Datasets/xl_sum_sample_val.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a0e4d",
   "metadata": {},
   "source": [
    "**All Categories: Untrained**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "828fd7c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#remember to push new model to huggingface repo after running prev cell so that this cell uses the most recent model!!!!!!!\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3ec36b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but you input_length is only 172. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=86)\n",
      "Your max_length is set to 256, but you input_length is only 123. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      "Your max_length is set to 256, but you input_length is only 221. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
      "Your max_length is set to 256, but you input_length is only 255. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=127)\n",
      "Your max_length is set to 256, but you input_length is only 56. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 256, but you input_length is only 151. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=75)\n",
      "Your max_length is set to 256, but you input_length is only 220. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
      "Your max_length is set to 256, but you input_length is only 219. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=109)\n",
      "Your max_length is set to 256, but you input_length is only 191. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=95)\n",
      "Your max_length is set to 256, but you input_length is only 240. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=120)\n",
      "Your max_length is set to 256, but you input_length is only 212. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=106)\n",
      "Your max_length is set to 256, but you input_length is only 227. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=113)\n",
      "Your max_length is set to 256, but you input_length is only 80. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "Your max_length is set to 256, but you input_length is only 163. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=81)\n",
      "Your max_length is set to 256, but you input_length is only 202. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=101)\n"
     ]
    }
   ],
   "source": [
    "bart_r1 = []\n",
    "bart_r2 = []\n",
    "bart_rL = []\n",
    "bart_rLs = []\n",
    "bart_chrf = []\n",
    "\n",
    "for i in range(int(len(df['text']))):\n",
    "    \n",
    "    candidate = summarizer(df['text'][i], \n",
    "                           truncation = True, #truncated to first 1024 words, because that is all the model can handle\n",
    "                           max_length = 256, # same as pegasus\n",
    "                           min_length = 0, \n",
    "                            )[0]\n",
    "    candidate = [candidate['summary_text']]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bart_r1.append(results['rouge1'])\n",
    "    bart_r2.append(results['rouge2'])\n",
    "    bart_rL.append(results['rougeL'])\n",
    "    bart_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    results = chrf.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bart_chrf.append(results['score'])\n",
    "    \n",
    "#     if i in np.arange(0, 2000, 100):\n",
    "#         data = {'rouge1': bart_r1, 'rouge2': bart_r2, 'rogueL': bart_rL, 'rogueLs': bart_rLs, 'chrf': bart_chrf}\n",
    "#         scores = pd.DataFrame(data)\n",
    "#         scores.to_csv(r'BART_trained_0_scores.csv', index=False)\n",
    "#         print(i)\n",
    "        \n",
    "# data = {'rouge1': bart_r1, 'rouge2': bart_r2, 'rogueL': bart_rL, 'rogueLs': bart_rLs,'chrf': bart_chrf}\n",
    "# scores = pd.DataFrame(data)\n",
    "# scores.to_csv(r'BART_trained_0_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e40379b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Article She tweeted that the clothes would be \"archived & expertly cared for in the spirit & love of Michael Jackson, his bravery, & fans worldwide\". The auction included a jacket worn during Jackson's Bad tour, that went for $240,000 (£148,000) and two crystal gloves. The items were all made by designers Dennis Tompkins and Michael Bush. Lady Gaga also tweeted a picture of herself and her bidding paddle at the auction. More than $5m (£3.1m) was raised by the sale, according to LA-based Julien's Auctions. Other items that went under the hammer included jackets from Michael Jackson's Dangerous and Thriller tours and a pair of jeans that went for $50,000 (£31,000). Some of the money raised by the auction is being donated to a guide dogs charity and a hospice in Las Vegas. American costume designers Michael Bush and Dennis Tompkins created thousands of original pieces for Michael Jackson during his career. However, despite Lady Gaga's assurances, some fans expressed their anger online, claiming that Jackson wanted his costumes to end up in a museum. The singer died on 25 June 2009 from an overdose of the powerful anaesthetic propofol. His personal doctor, Conrad Murray, was later found guilty of his involuntary manslaughter and sentenced to four years in jail. Follow @BBCNewsbeat on Twitter\n",
      "Last Reference Summary ['Lady Gaga has bought 55 items at an auction of Michael Jackson costumes in Los Angeles.']\n",
      "Last Candidate Summary ['She tweeted that the clothes would be \"archived & expertly cared for in the spirit & love of Michael Jackson, his bravery, & fans worldwide\". The auction included a jacket worn during Jackson\\'s Bad tour, that went for $240,000 (£148,000) and two crystal gloves. The items were all made by designers Dennis Tompkins and Michael Bush. Lady Gaga also tweeted a picture of herself and her bidding, saying that she would donate the items to a charity in Las Vegas, Nevada, to help raise money for the victims of the Michael Jackson drug overdose. The singer died on 25 June 2009 from an overdose of propofol at the auction. More than $5m (£3.1m) was raised by the sale, according to LA-based Julien\\'s Auctions. Other items that went under the hammer included jackets from Michael Jackson\\'s Dangerous and Thriller tours and a pair of jeans that went at $50,00 (£31,000). Some of the money raised by a auction is being donated to a guide dogs charity and a hospice in Las Las Vegas. American costume designers Michael Bush and DennisTompkins created thousands of original pieces for Michael Jackson during his career. However, despite Lady Gaga\\'s assurances,']\n"
     ]
    }
   ],
   "source": [
    "print('Last Article', df['text'][i])\n",
    "print('Last Reference Summary', ref)\n",
    "print('Last Candidate Summary', candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1723015b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.12478740483411083\n",
      "rouge2 average : 0.02582721200404865\n",
      "rougeL average : 0.08001844569806658\n",
      "rougeLs average : 0.08001844569806658\n",
      "chrf average : 19.999522510333566\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bart_r1))\n",
    "print('rouge2 average :', np.mean(bart_r2))\n",
    "print('rougeL average :', np.mean(bart_rL))\n",
    "print('rougeLs average :', np.mean(bart_rLs))\n",
    "print('chrf average :', np.mean(bart_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdae58a-e4c0-4a6c-8f67-b8309a24c665",
   "metadata": {},
   "source": [
    "**All Categories: Model 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41489617-caa3-421b-861a-7716cb853985",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 transformers/examples/pytorch/summarization/run_summarization.py \\\n",
    "    --model_name_or_path=facebook/bart-base \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_file 'w266_project/xl_sum_sample_train.csv' \\\n",
    "    --validation_file 'w266_project/xl_sum_sample_val.csv' \\\n",
    "    --text_column text \\\n",
    "    --summary_column summary \\\n",
    "    --push_to_hub=True \\\n",
    "    --max_source_length 128 \\\n",
    "    --max_target_length 32 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size=32 \\\n",
    "    --per_device_eval_batch_size=32 \\\n",
    "    --output_dir='w266_project/finetuned-BART-all-categories/model 0/finetuned-BART-all-categories' \\\n",
    "    --overwrite_output_dir=True \\\n",
    "    --predict_with_generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1f378c-07b9-48ec-b77c-c4bc558143af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██| 1.74k/1.74k [00:00<00:00, 246kB/s]\n",
      "Downloading pytorch_model.bin: 100%|█████████| 558M/558M [00:06<00:00, 85.0MB/s]\n",
      "Downloading (…)neration_config.json: 100%|█████| 297/297 [00:00<00:00, 47.4kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████| 384/384 [00:00<00:00, 158kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|████| 798k/798k [00:00<00:00, 101MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|███| 456k/456k [00:00<00:00, 68.6MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██| 2.11M/2.11M [00:00<00:00, 112MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████| 280/280 [00:00<00:00, 133kB/s]\n"
     ]
    }
   ],
   "source": [
    "#remember to push new model to huggingface repo after running prev cell so that this cell uses the most recent model!!!!!!!\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"arisanguyen/finetuned-BART-all-categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b8a9c-6385-4f29-b3fc-caa2bbbd1c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bart_r1 = []\n",
    "bart_r2 = []\n",
    "bart_rL = []\n",
    "bart_rLs = []\n",
    "bart_chrf = []\n",
    "\n",
    "for i in range(int(len(df['text']))):\n",
    "    \n",
    "    #art = ' '.join(df['article'][i].split(' ')[:1024]) #truncated to first 1024 words, because that is all the model can handle\n",
    "    \n",
    "    candidate = summarizer(df['text'][i], \n",
    "                           truncation = True, #truncated to first 1024 words, because that is all the model can handle\n",
    "                             #max_length=130, min_length=30, do_sample=False\n",
    "                            )[0]\n",
    "    candidate = [candidate['summary_text']]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bart_r1.append(results['rouge1'])\n",
    "    bart_r2.append(results['rouge2'])\n",
    "    bart_rL.append(results['rougeL'])\n",
    "    bart_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    results = chrf.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bart_chrf.append(results['score'])\n",
    "    \n",
    "#     if i in np.arange(0, 2000, 100):\n",
    "#         data = {'rouge1': bart_r1, 'rouge2': bart_r2, 'rogueL': bart_rL, 'rogueLs': bart_rLs, 'chrf': bart_chrf}\n",
    "#         scores = pd.DataFrame(data)\n",
    "#         scores.to_csv(r'BART_trained_0_scores.csv', index=False)\n",
    "#         print(i)\n",
    "        \n",
    "# data = {'rouge1': bart_r1, 'rouge2': bart_r2, 'rogueL': bart_rL, 'rogueLs': bart_rLs,'chrf': bart_chrf}\n",
    "# scores = pd.DataFrame(data)\n",
    "# scores.to_csv(r'BART_trained_0_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ed12cd2-800e-43b2-b5d3-445b531d7326",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Article She tweeted that the clothes would be \"archived & expertly cared for in the spirit & love of Michael Jackson, his bravery, & fans worldwide\". The auction included a jacket worn during Jackson's Bad tour, that went for $240,000 (£148,000) and two crystal gloves. The items were all made by designers Dennis Tompkins and Michael Bush. Lady Gaga also tweeted a picture of herself and her bidding paddle at the auction. More than $5m (£3.1m) was raised by the sale, according to LA-based Julien's Auctions. Other items that went under the hammer included jackets from Michael Jackson's Dangerous and Thriller tours and a pair of jeans that went for $50,000 (£31,000). Some of the money raised by the auction is being donated to a guide dogs charity and a hospice in Las Vegas. American costume designers Michael Bush and Dennis Tompkins created thousands of original pieces for Michael Jackson during his career. However, despite Lady Gaga's assurances, some fans expressed their anger online, claiming that Jackson wanted his costumes to end up in a museum. The singer died on 25 June 2009 from an overdose of the powerful anaesthetic propofol. His personal doctor, Conrad Murray, was later found guilty of his involuntary manslaughter and sentenced to four years in jail. Follow @BBCNewsbeat on Twitter\n",
      "Last Reference Summary ['Lady Gaga has bought 55 items at an auction of Michael Jackson costumes in Los Angeles.']\n",
      "Last Candidate Summary [\"Lady Gaga has announced that she will be auctioning off a collection of Michael Jackson's costumes at an auction in Las Vegas.\"]\n"
     ]
    }
   ],
   "source": [
    "print('Last Article', df['text'][i])\n",
    "print('Last Reference Summary', ref)\n",
    "print('Last Candidate Summary', candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ae281a4-5d2e-48fa-a4a4-0a500d69e539",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.3192639042917483\n",
      "rouge2 average : 0.10466960084268205\n",
      "rougeL average : 0.24501046685959502\n",
      "rougeLs average : 0.24501046685959502\n",
      "chrf average : 27.774860738085867\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bart_r1))\n",
    "print('rouge2 average :', np.mean(bart_r2))\n",
    "print('rougeL average :', np.mean(bart_rL))\n",
    "print('rougeLs average :', np.mean(bart_rLs))\n",
    "print('chrf average :', np.mean(bart_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d130492-21d3-4af9-b626-9fa36e271252",
   "metadata": {},
   "source": [
    "**All Categories: Model 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea7edaa-d918-4c59-8750-f70dc45e7527",
   "metadata": {},
   "source": [
    "Added revision version. Set the max source length to 512, target length to 256, and number of epochs to 62 (~2k steps total) to be same as PEGASUS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c9182eab-2816-4040-82cb-9b85b74ceded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/ubuntu/w266/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "04/03/2023 05:53:59 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "04/03/2023 05:53:59 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/runs/Apr03_05-53-59_ip-172-31-55-240,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=62.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "04/03/2023 05:53:59 - INFO - datasets.builder - Using custom data configuration default-0fb02eb9fe37de5d\n",
      "04/03/2023 05:53:59 - INFO - datasets.info - Loading Dataset Infos from /home/ubuntu/w266/lib/python3.8/site-packages/datasets/packaged_modules/csv\n",
      "04/03/2023 05:53:59 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "04/03/2023 05:53:59 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/csv/default-0fb02eb9fe37de5d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
      "04/03/2023 05:53:59 - WARNING - datasets.builder - Found cached dataset csv (/home/ubuntu/.cache/huggingface/datasets/csv/default-0fb02eb9fe37de5d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "04/03/2023 05:53:59 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/csv/default-0fb02eb9fe37de5d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 683.78it/s]\n",
      "[INFO|configuration_utils.py:668] 2023-04-03 05:53:59,462 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-04-03 05:53:59,466 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:494] 2023-04-03 05:53:59,521 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:668] 2023-04-03 05:53:59,545 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-04-03 05:53:59,545 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-03 05:53:59,595 >> loading file vocab.json from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-03 05:53:59,595 >> loading file merges.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-03 05:53:59,595 >> loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-03 05:53:59,595 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-03 05:53:59,595 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-03 05:53:59,595 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:668] 2023-04-03 05:53:59,595 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-04-03 05:53:59,596 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2451] 2023-04-03 05:53:59,671 >> loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:575] 2023-04-03 05:53:59,972 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.28.0.dev0\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3100] 2023-04-03 05:54:02,356 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3108] 2023-04-03 05:54:02,356 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "[INFO|modeling_utils.py:2753] 2023-04-03 05:54:02,392 >> Generation config file not found, using a generation config created from the model config.\n",
      "Running tokenizer on train dataset:   0%|       | 0/1000 [00:00<?, ? examples/s]04/03/2023 05:54:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/csv/default-0fb02eb9fe37de5d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-765ebb88f12389b1.arrow\n",
      "/home/ubuntu/w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories is already a clone of https://huggingface.co/arisanguyen/finetuned-BART-all-categories. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "04/03/2023 05:54:04 - WARNING - huggingface_hub.repository - /home/ubuntu/w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories is already a clone of https://huggingface.co/arisanguyen/finetuned-BART-all-categories. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "/home/ubuntu/w266/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1746] 2023-04-03 05:54:06,546 >> ***** Running training *****\n",
      "[INFO|trainer.py:1747] 2023-04-03 05:54:06,546 >>   Num examples = 1000\n",
      "[INFO|trainer.py:1748] 2023-04-03 05:54:06,546 >>   Num Epochs = 62\n",
      "[INFO|trainer.py:1749] 2023-04-03 05:54:06,546 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1750] 2023-04-03 05:54:06,546 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1751] 2023-04-03 05:54:06,546 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1752] 2023-04-03 05:54:06,546 >>   Total optimization steps = 1984\n",
      "[INFO|trainer.py:1753] 2023-04-03 05:54:06,547 >>   Number of trainable parameters = 139420416\n",
      "  0%|                                                  | 0/1984 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-04-03 05:54:06,570 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 1.1076, 'learning_rate': 3.7399193548387094e-05, 'epoch': 15.62}       \n",
      " 25%|█████████                           | 500/1984 [2:17:58<6:57:24, 16.88s/it][INFO|trainer.py:2821] 2023-04-03 08:12:05,015 >> Saving model checkpoint to w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-500\n",
      "[INFO|configuration_utils.py:457] 2023-04-03 08:12:05,016 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-500/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-03 08:12:05,017 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-500/generation_config.json\n",
      "[INFO|modeling_utils.py:1812] 2023-04-03 08:12:05,568 >> Model weights saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-03 08:12:05,569 >> tokenizer config file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2177] 2023-04-03 08:12:05,569 >> Special tokens file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-03 08:12:10,589 >> tokenizer config file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2177] 2023-04-03 08:12:10,589 >> Special tokens file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/special_tokens_map.json\n",
      "{'loss': 0.1465, 'learning_rate': 2.4798387096774196e-05, 'epoch': 31.25}       \n",
      " 50%|█████████████████▋                 | 1000/1984 [4:37:02<4:36:35, 16.86s/it][INFO|trainer.py:2821] 2023-04-03 10:31:09,045 >> Saving model checkpoint to w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-1000\n",
      "[INFO|configuration_utils.py:457] 2023-04-03 10:31:09,046 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-1000/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-03 10:31:09,047 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-1000/generation_config.json\n",
      "[INFO|modeling_utils.py:1812] 2023-04-03 10:31:09,601 >> Model weights saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-03 10:31:09,602 >> tokenizer config file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2177] 2023-04-03 10:31:09,602 >> Special tokens file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-03 10:31:16,636 >> tokenizer config file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2177] 2023-04-03 10:31:16,637 >> Special tokens file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/special_tokens_map.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "04/03/2023 10:31:37 - WARNING - huggingface_hub.repository - Several commits (2) will be pushed upstream.\n",
      "{'loss': 0.0467, 'learning_rate': 1.2197580645161291e-05, 'epoch': 46.88}       \n",
      " 76%|██████████████████████████▍        | 1500/1984 [6:56:12<2:21:46, 17.58s/it][INFO|trainer.py:2821] 2023-04-03 12:50:18,573 >> Saving model checkpoint to w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-1500\n",
      "[INFO|configuration_utils.py:457] 2023-04-03 12:50:18,574 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-1500/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-03 12:50:18,575 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-1500/generation_config.json\n",
      "[INFO|modeling_utils.py:1812] 2023-04-03 12:50:19,124 >> Model weights saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-03 12:50:19,125 >> tokenizer config file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2177] 2023-04-03 12:50:19,125 >> Special tokens file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-03 12:50:26,241 >> tokenizer config file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2177] 2023-04-03 12:50:26,246 >> Special tokens file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/special_tokens_map.json\n",
      "Several commits (3) will be pushed upstream.\n",
      "04/03/2023 12:50:46 - WARNING - huggingface_hub.repository - Several commits (3) will be pushed upstream.\n",
      "100%|█████████████████████████████████████| 1984/1984 [9:10:40<00:00, 12.96s/it][INFO|trainer.py:2016] 2023-04-03 15:04:46,736 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 33040.1899, 'train_samples_per_second': 1.877, 'train_steps_per_second': 0.06, 'train_loss': 0.3340432326639852, 'epoch': 62.0}\n",
      "100%|█████████████████████████████████████| 1984/1984 [9:10:40<00:00, 16.65s/it]\n",
      "[INFO|trainer.py:2821] 2023-04-03 15:04:46,740 >> Saving model checkpoint to w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories\n",
      "[INFO|configuration_utils.py:457] 2023-04-03 15:04:46,741 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-03 15:04:46,742 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/generation_config.json\n",
      "[INFO|modeling_utils.py:1812] 2023-04-03 15:04:50,507 >> Model weights saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-03 15:04:50,508 >> tokenizer config file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2177] 2023-04-03 15:04:50,508 >> Special tokens file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/special_tokens_map.json\n",
      "[INFO|trainer.py:2821] 2023-04-03 15:04:50,610 >> Saving model checkpoint to w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories\n",
      "[INFO|configuration_utils.py:457] 2023-04-03 15:04:50,611 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-03 15:04:50,612 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/generation_config.json\n",
      "[INFO|modeling_utils.py:1812] 2023-04-03 15:04:54,711 >> Model weights saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-03 15:04:54,711 >> tokenizer config file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2177] 2023-04-03 15:04:54,712 >> Special tokens file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/special_tokens_map.json\n",
      "Several commits (4) will be pushed upstream.\n",
      "04/03/2023 15:05:17 - WARNING - huggingface_hub.repository - Several commits (4) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n",
      "04/03/2023 15:05:17 - WARNING - huggingface_hub.repository - The progress bars may be unreliable.\n",
      "Upload file pytorch_model.bin:   0%|                 | 1.00/532M [00:00<?, ?B/s]\n",
      "Upload file pytorch_model.bin:  89%|████████ | 473M/532M [00:09<00:00, 62.0MB/s]\u001b[ATo https://huggingface.co/arisanguyen/finetuned-BART-all-categories\n",
      "   52e95ea..a33b58b  model_1 -> model_1\n",
      "\n",
      "04/03/2023 15:05:29 - WARNING - huggingface_hub.repository - To https://huggingface.co/arisanguyen/finetuned-BART-all-categories\n",
      "   52e95ea..a33b58b  model_1 -> model_1\n",
      "\n",
      "Upload file pytorch_model.bin: 100%|█████████| 532M/532M [00:10<00:00, 55.7MB/s]\n",
      "\n",
      "Upload file runs/Apr03_05-53-59_ip-172-31-55-240/events.out.tfevents.1680501246.\u001b[A\n",
      "Upload file runs/Apr03_05-53-59_ip-172-31-55-240/events.out.tfevents.1680501246.\u001b[A\n",
      "[INFO|modelcard.py:451] 2023-04-03 15:05:31,653 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}}\n",
      "Several commits (5) will be pushed upstream.\n",
      "04/03/2023 15:05:33 - WARNING - huggingface_hub.repository - Several commits (5) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n",
      "04/03/2023 15:05:33 - WARNING - huggingface_hub.repository - The progress bars may be unreliable.\n",
      "To https://huggingface.co/arisanguyen/finetuned-BART-all-categories\n",
      "   a33b58b..ad02ca2  model_1 -> model_1\n",
      "\n",
      "04/03/2023 15:05:33 - WARNING - huggingface_hub.repository - To https://huggingface.co/arisanguyen/finetuned-BART-all-categories\n",
      "   a33b58b..ad02ca2  model_1 -> model_1\n",
      "\n",
      "***** train metrics *****\n",
      "  epoch                    =       62.0\n",
      "  train_loss               =      0.334\n",
      "  train_runtime            = 9:10:40.18\n",
      "  train_samples            =       1000\n",
      "  train_samples_per_second =      1.877\n",
      "  train_steps_per_second   =       0.06\n",
      "[INFO|trainer.py:2821] 2023-04-03 15:05:37,406 >> Saving model checkpoint to w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories\n",
      "[INFO|configuration_utils.py:457] 2023-04-03 15:05:37,407 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/config.json\n",
      "[INFO|configuration_utils.py:362] 2023-04-03 15:05:37,409 >> Configuration saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/generation_config.json\n",
      "[INFO|modeling_utils.py:1812] 2023-04-03 15:05:41,199 >> Model weights saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-04-03 15:05:41,200 >> tokenizer config file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2177] 2023-04-03 15:05:41,200 >> Special tokens file saved in w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories/special_tokens_map.json\n",
      "Several commits (6) will be pushed upstream.\n",
      "04/03/2023 15:05:46 - WARNING - huggingface_hub.repository - Several commits (6) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n",
      "04/03/2023 15:05:46 - WARNING - huggingface_hub.repository - The progress bars may be unreliable.\n",
      "To https://huggingface.co/arisanguyen/finetuned-BART-all-categories\n",
      "   ad02ca2..9d40da8  model_1 -> model_1\n",
      "\n",
      "04/03/2023 15:05:47 - WARNING - huggingface_hub.repository - To https://huggingface.co/arisanguyen/finetuned-BART-all-categories\n",
      "   ad02ca2..9d40da8  model_1 -> model_1\n",
      "\n",
      "[INFO|modelcard.py:451] 2023-04-03 15:05:50,849 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Summarization', 'type': 'summarization'}}\n"
     ]
    }
   ],
   "source": [
    "!python3 transformers/examples/pytorch/summarization/run_summarization.py \\\n",
    "    --model_name_or_path=facebook/bart-base \\\n",
    "    --do_train \\\n",
    "    --train_file 'w266_project/xl_sum_sample_train.csv' \\\n",
    "    --text_column text \\\n",
    "    --summary_column summary \\\n",
    "    --max_source_length 512 \\\n",
    "    --max_target_length 256 \\\n",
    "    --num_train_epochs 62 \\\n",
    "    --per_device_train_batch_size=32 \\\n",
    "    --push_to_hub=True \\\n",
    "    --output_dir='w266_project/finetuned-BART-all-categories/model 1/finetuned-BART-all-categories' \\\n",
    "    --overwrite_output_dir=True \\\n",
    "    --predict_with_generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "06c6c9a2-07fa-48ac-bb37-2395345a49a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|█████████| 558M/558M [00:07<00:00, 70.5MB/s]\n",
      "Downloading (…)del_1/tokenizer.json: 100%|██| 2.11M/2.11M [00:00<00:00, 191MB/s]\n"
     ]
    }
   ],
   "source": [
    "#remember to push new model to huggingface repo after running prev cell so that this cell uses the most recent model!!!!!!!\n",
    "\n",
    "# model = AutoModel.from_pretrained(\n",
    "#     \"julien-c/EsperBERTo-small\", revision=\"v2.0.1\"  # tag name, or branch name, or commit hash\n",
    "# )\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"arisanguyen/finetuned-BART-all-categories\", revision = \"model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "001c815f-2455-4d9c-8e83-fdeb7e200a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_default_args(summarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9062b049-9393-44a7-86ec-c5d0cdacdba9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/w266/lib/python3.8/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but you input_length is only 172. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=86)\n",
      "Your max_length is set to 256, but you input_length is only 123. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      "Your max_length is set to 256, but you input_length is only 221. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
      "Your max_length is set to 256, but you input_length is only 255. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=127)\n",
      "Your max_length is set to 256, but you input_length is only 56. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 256, but you input_length is only 151. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=75)\n",
      "Your max_length is set to 256, but you input_length is only 220. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
      "Your max_length is set to 256, but you input_length is only 219. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=109)\n",
      "Your max_length is set to 256, but you input_length is only 191. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=95)\n",
      "Your max_length is set to 256, but you input_length is only 240. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=120)\n",
      "Your max_length is set to 256, but you input_length is only 212. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=106)\n",
      "Your max_length is set to 256, but you input_length is only 227. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=113)\n",
      "Your max_length is set to 256, but you input_length is only 80. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "Your max_length is set to 256, but you input_length is only 163. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=81)\n",
      "Your max_length is set to 256, but you input_length is only 202. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=101)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Article She tweeted that the clothes would be \"archived & expertly cared for in the spirit & love of Michael Jackson, his bravery, & fans worldwide\". The auction included a jacket worn during Jackson's Bad tour, that went for $240,000 (£148,000) and two crystal gloves. The items were all made by designers Dennis Tompkins and Michael Bush. Lady Gaga also tweeted a picture of herself and her bidding paddle at the auction. More than $5m (£3.1m) was raised by the sale, according to LA-based Julien's Auctions. Other items that went under the hammer included jackets from Michael Jackson's Dangerous and Thriller tours and a pair of jeans that went for $50,000 (£31,000). Some of the money raised by the auction is being donated to a guide dogs charity and a hospice in Las Vegas. American costume designers Michael Bush and Dennis Tompkins created thousands of original pieces for Michael Jackson during his career. However, despite Lady Gaga's assurances, some fans expressed their anger online, claiming that Jackson wanted his costumes to end up in a museum. The singer died on 25 June 2009 from an overdose of the powerful anaesthetic propofol. His personal doctor, Conrad Murray, was later found guilty of his involuntary manslaughter and sentenced to four years in jail. Follow @BBCNewsbeat on Twitter\n",
      "Last Reference Summary ['Lady Gaga has bought 55 items at an auction of Michael Jackson costumes in Los Angeles.']\n",
      "Last Candidate Summary ['Lady Gaga has auctioned off hundreds of items worth more than $190,000 (£152,000) at an auction in Las Vegas.']\n",
      "rouge1 average : 0.31336770582535894\n",
      "rouge2 average : 0.09975946851022606\n",
      "rougeL average : 0.23881053257054677\n",
      "rougeLs average : 0.23881053257054677\n",
      "chrf average : 28.86864045130219\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./w266_project/xl_sum_sample_val.csv')\n",
    "\n",
    "bart_r1 = []\n",
    "bart_r2 = []\n",
    "bart_rL = []\n",
    "bart_rLs = []\n",
    "bart_chrf = []\n",
    "\n",
    "for i in range(int(len(df['text']))):\n",
    "    \n",
    "    #art = ' '.join(df['article'][i].split(' ')[:1024]) #truncated to first 1024 words, because that is all the model can handle\n",
    "    \n",
    "    candidate = summarizer(df['text'][i], \n",
    "                           truncation = True, #truncated to first 1024 words, because that is all the model can handle\n",
    "                           max_length = 256, # same as pegasus\n",
    "                           min_length = 0, \n",
    "                             #max_length=130, min_length=30, do_sample=False\n",
    "                            )[0]\n",
    "    candidate = [candidate['summary_text']]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bart_r1.append(results['rouge1'])\n",
    "    bart_r2.append(results['rouge2'])\n",
    "    bart_rL.append(results['rougeL'])\n",
    "    bart_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    results = chrf.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bart_chrf.append(results['score'])\n",
    "    \n",
    "    if i in np.arange(0, 2000, 100):\n",
    "        data = {'rouge1': bart_r1, 'rouge2': bart_r2, 'rogueL': bart_rL, 'rogueLs': bart_rLs, 'chrf': bart_chrf}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BART_trained_1_scores.csv', index=False)\n",
    "        print(i)\n",
    "        \n",
    "data = {'rouge1': bart_r1, 'rouge2': bart_r2, 'rogueL': bart_rL, 'rogueLs': bart_rLs,'chrf': bart_chrf}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BART_trained_1_scores.csv', index=False)\n",
    "\n",
    "print('Last Article', df['text'][i])\n",
    "print('Last Reference Summary', ref)\n",
    "print('Last Candidate Summary', candidate)\n",
    "\n",
    "print('rouge1 average :', np.mean(bart_r1))\n",
    "print('rouge2 average :', np.mean(bart_r2))\n",
    "print('rougeL average :', np.mean(bart_rL))\n",
    "print('rougeLs average :', np.mean(bart_rLs))\n",
    "print('chrf average :', np.mean(bart_chrf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
