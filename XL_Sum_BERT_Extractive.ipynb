{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d870553",
   "metadata": {},
   "source": [
    "**Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dacdaf3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "#let's make longer output readable without horizontal scrolling\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f29e293",
   "metadata": {},
   "source": [
    "**Necessary Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc5f44ab-39ad-49be-a367-c2ff78ac845a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def get_default_args(func):\n",
    "    signature = inspect.signature(func)\n",
    "    return {\n",
    "        k: v.default\n",
    "        for k, v in signature.parameters.items()\n",
    "        if v.default is not inspect.Parameter.empty\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f96edd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e922d01-d029-42ee-bf38-6765115f1e04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chrf = evaluate.load(\"chrf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586a6d00",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "163d7bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Datasets'\n",
      "/home/ubuntu/w266_project/Datasets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/w266_project/Datasets'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n",
    "%cd Datasets\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cce6327",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('xl_sum_sample_val.csv')\n",
    "df.rename(columns={\"text\": \"article\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eea246fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.read_csv('xl_sum_sample_test.csv')\n",
    "dft.rename(columns={\"text\": \"article\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f872e-a7c8-4058-8ba0-7935db2207b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Default Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a312a65-1c63-4251-8553-c66959b057a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8edfe31-ebe3-4c38-b8ef-7df7a013d1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'bert-large-uncased',\n",
       " 'custom_model': None,\n",
       " 'custom_tokenizer': None,\n",
       " 'hidden': -2,\n",
       " 'reduce_option': 'mean',\n",
       " 'sentence_handler': <summarizer.sentence_handler.SentenceHandler at 0x7f081a6b7760>,\n",
       " 'random_state': 12345,\n",
       " 'hidden_concat': False}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_default_args(Summarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8c09bea-3299-40b8-a6a2-a7a38c9c0c26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ratio': 0.2,\n",
       " 'min_length': 40,\n",
       " 'max_length': 600,\n",
       " 'use_first': True,\n",
       " 'algorithm': 'kmeans',\n",
       " 'num_sentences': None,\n",
       " 'return_as_list': False}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_default_args(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4266569",
   "metadata": {},
   "source": [
    "**BERT Extractive Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c76d1-50eb-4560-b78c-3f05e2c0fe3e",
   "metadata": {},
   "source": [
    "Model 0: Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89fd59b8-44ed-4497-9f64-f0d14abbdecd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af9232aa-5fa4-4beb-a11a-e25092a1cdda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(int(len(df['article']))):\n",
    "    \n",
    "    candidate = model(df['article'][i],\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "#     if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "#         data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "#         scores = pd.DataFrame(data)\n",
    "#         scores.to_csv(r'BERT_0_scores.csv', index=False)\n",
    "#         print(i)\n",
    "\n",
    "# data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "# scores = pd.DataFrame(data)\n",
    "# scores.to_csv(r'BERT_0_scores.csv', index=False)\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f3230aee-fe28-4099-ae35-b9fd18f5d2f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.1600066478049013\n",
      "rouge2 average : 0.026148022779671355\n",
      "rougeL average : 0.1076876885358689\n",
      "rougeLs average : 0.1076876885358689\n",
      "chrf average: 23.9904498468375\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d9ce16-2e35-453d-8c3a-f1e1d354c68c",
   "metadata": {},
   "source": [
    "Model 1. Adjusted min_length, max_length, num_sentences (which overrides ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79dde227-1016-4f35-a0cd-b6a2f3d5f587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from summarizer import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "462afc0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using bert-base instead of bert-large to reduce run times\n",
    "model = Summarizer(model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2595b5cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(len(df['article'])):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences (average number of sentences per summary in train set)\n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = 2, \n",
    "                      min_length = 0,\n",
    "                      max_length = 256,        \n",
    "                      ratio = None,\n",
    "                      use_first = None,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "#     if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "#         data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "#         scores = pd.DataFrame(data)\n",
    "#         scores.to_csv(r'BERT_1_scores.csv', index=False)\n",
    "#         print(i)\n",
    "        \n",
    "# data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "# scores = pd.DataFrame(data)\n",
    "# scores.to_csv(r'BERT_1_scores.csv', index=False)\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "97555fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.18574665627167028\n",
      "rouge2 average : 0.02606738930501224\n",
      "rougeL average : 0.12140470038266893\n",
      "rougeLs average : 0.12140470038266893\n",
      "chrf average: 26.329590625959923\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec631b8-e0a8-40ac-8f16-fe3094b118cc",
   "metadata": {},
   "source": [
    "Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96727286-a2e4-4e08-a014-1d205a55dc44",
   "metadata": {},
   "source": [
    "2. Made num_sentences based off number of clusters instead of average "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68b71359-02c7-4236-b22e-5eb077d02a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer(model='bert-base-uncased',\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e007312b-1348-45d4-ab1b-f32c270bb415",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(len(df['article'])):\n",
    "\n",
    "    res = model.calculate_optimal_k(df['article'][i], k_max=10)\n",
    "    \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = res, # number of sentences determined by number of clusters\n",
    "                      min_length = 0,\n",
    "                      max_length = 256,        \n",
    "                      ratio = None,\n",
    "                      use_first = None,)\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "#     bert_r1.append(results['rouge1'])\n",
    "#     bert_r2.append(results['rouge2'])\n",
    "#     bert_rL.append(results['rougeL'])\n",
    "#     bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "#     if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "#         data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "#         scores = pd.DataFrame(data)\n",
    "#         scores.to_csv(r'BERT_2_scores.csv', index=False)\n",
    "#         print(i)\n",
    "        \n",
    "# data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "# scores = pd.DataFrame(data)\n",
    "# scores.to_csv(r'BERT_2_scores.csv', index=False)\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "701447ab-000a-4aea-865a-c5244b0f342c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : nan\n",
      "rouge2 average : nan\n",
      "rougeL average : nan\n",
      "rougeLs average : nan\n",
      "chrf average: 25.487034086359877\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd8b62-7d3b-4a03-af4c-c9ecec023faa",
   "metadata": {},
   "source": [
    "3. Setting number of sentences with clusters didn't help in second model, so we are reverting back to first model but setting use_first = False bc many examples have meta data in first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b74b6df2-673c-4083-9939-6e30b722df4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from summarizer import Summarizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model = Summarizer(model='bert-base-uncased',\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "815245f2-2673-4ac4-a2c3-4085e5b920aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(len(df['article'])):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = 2, \n",
    "                      min_length = 0,\n",
    "                      max_length = 256,        \n",
    "                      ratio = None,\n",
    "                      use_first = False,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "                            \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "#     if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "#         data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "#         scores = pd.DataFrame(data)\n",
    "#         scores.to_csv(r'BERT_3_scores.csv', index=False)\n",
    "#         print(i)\n",
    "\n",
    "# data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "# scores = pd.DataFrame(data)\n",
    "# scores.to_csv(r'BERT_3_scores.csv', index=False)\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29baf035-e761-40da-abc0-9dd6ac2de9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.18574665627167028\n",
      "rouge2 average : 0.02606738930501224\n",
      "rougeL average : 0.12140470038266893\n",
      "rougeLs average : 0.12140470038266893\n",
      "chrf average: 26.329590625959923\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1715530b-d2c9-45bb-9152-423e3acc7394",
   "metadata": {},
   "source": [
    "4. Setting use_first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce1bd98a-1738-4785-8d76-d463896cfc53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer(model='bert-base-uncased',\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "72ef153b-ad2e-4059-b24a-0dd2db495bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(len(df['article'])):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = 2, \n",
    "                      min_length = 0,\n",
    "                      max_length = 256,        \n",
    "                      ratio = None,\n",
    "                      use_first = True,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "#     if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "#         data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "#         scores = pd.DataFrame(data)\n",
    "#         scores.to_csv(r'BERT_3_scores.csv', index=False)\n",
    "#         print(i)\n",
    "\n",
    "# data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "# scores = pd.DataFrame(data)\n",
    "# scores.to_csv(r'BERT_3_scores.csv', index=False)\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "082a64a6-0e87-4e36-8b47-b65f4942e139",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.18420413444525\n",
      "rouge2 average : 0.029384449091168774\n",
      "rougeL average : 0.1187318906362462\n",
      "rougeLs average : 0.1187318906362462\n",
      "chrf average: 26.276875760389853\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a566379-110d-4741-8439-1738fa5e3389",
   "metadata": {},
   "source": [
    "5. Use bert-large-uncased w/ best from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "87a140b7-6255-4b2a-93e8-fbb10dcf912d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer(model='bert-large-uncased',\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2934abbd-5dae-4f94-8e90-dc2e45a6fe6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(len(df['article'])):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = 2, \n",
    "                      min_length = 0,\n",
    "                      max_length = 256,        \n",
    "                      ratio = None,\n",
    "                      use_first = False,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "#     if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "#         data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "#         scores = pd.DataFrame(data)\n",
    "#         scores.to_csv(r'BERT_4_scores.csv', index=False)\n",
    "#         print(i)\n",
    "\n",
    "# data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "# scores = pd.DataFrame(data)\n",
    "# scores.to_csv(r'BERT_4_scores.csv', index=False)\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7192f7f4-028a-4abc-aa1a-925a1a8c3616",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.18045099952614682\n",
      "rouge2 average : 0.02579665252992228\n",
      "rougeL average : 0.1260598528415354\n",
      "rougeLs average : 0.1260598528415354\n",
      "chrf average: 25.93990117052502\n"
     ]
    }
   ],
   "source": [
    "# this model took longer and performed worse\n",
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cab172-dc0a-4962-990e-cefbec9f1366",
   "metadata": {},
   "source": [
    "6. Reduce option median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e5fa0c9-a090-4dff-b250-459de6983da5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer(model='bert-base-uncased', reduce_option = 'median'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9cad742a-6f5b-4add-b9b3-a6ab0c856630",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(len(df['article'])):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = 2, \n",
    "                      min_length = 0,\n",
    "                      max_length = 256,        \n",
    "                      ratio = None,\n",
    "                      use_first = None,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "#     if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "#         data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "#         scores = pd.DataFrame(data)\n",
    "#         scores.to_csv(r'BERT_4_scores.csv', index=False)\n",
    "#         print(i)\n",
    "\n",
    "# data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "# scores = pd.DataFrame(data)\n",
    "# scores.to_csv(r'BERT_6_scores.csv', index=False)\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "81108fe5-f4f9-4509-ade6-75b9cb09815a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.18240691681643514\n",
      "rouge2 average : 0.02531288951923227\n",
      "rougeL average : 0.11788137317284689\n",
      "rougeLs average : 0.11788137317284689\n",
      "chrf average: 25.955566975088626\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd379e9-9b75-49c2-a336-f132b63281e1",
   "metadata": {},
   "source": [
    "**7. Reduce option max**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "382d244f-a422-49b0-b141-0a7b55c4c163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer(model='bert-base-uncased', reduce_option = 'max'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f9a41-6fd4-4320-b51c-e026c01e01a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(len(df['article'])):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = 2, \n",
    "                      min_length = 0,\n",
    "                      max_length = 256,        \n",
    "                      ratio = None,\n",
    "                      use_first = None,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "#     if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "#         data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "#         scores = pd.DataFrame(data)\n",
    "#         scores.to_csv(r'BERT_7_scores.csv', index=False)\n",
    "#         print(i)\n",
    "\n",
    "# data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "# scores = pd.DataFrame(data)\n",
    "# scores.to_csv(r'BERT_7_scores.csv', index=False)\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd8b50-8555-4b77-9a56-edc169ccc0eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fbb598-7393-4006-8886-707879a490e1",
   "metadata": {},
   "source": [
    "**Best BERT Extractive model applied to one category**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05fdfde-349d-4598-b490-1cbd3fda6c75",
   "metadata": {},
   "source": [
    "**Category 1: uk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9549992c-d3d4-42d8-a3a2-57b774e36e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rouge1 average: ', np.mean(bert_r1))\n",
    "print('rouge2 average: ', np.mean(bert_r2))\n",
    "print('rougeL average: ', np.mean(bert_rL))\n",
    "print('rougeLs average:', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ff031d-c333-4c0e-a319-6cba1f8b5da6",
   "metadata": {},
   "source": [
    "**Category 2: world**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5d47e-fd5a-4b36-a84b-cdce83404763",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rouge1 average: ', np.mean(bert_r1))\n",
    "print('rouge2 average: ', np.mean(bert_r2))\n",
    "print('rougeL average: ', np.mean(bert_rL))\n",
    "print('rougeLs average:', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c3dcc-ecb5-434c-979f-3e60174819cf",
   "metadata": {},
   "source": [
    "**Category 3: business**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56801ea6-6e55-41e7-b984-c38c67221ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rouge1 average: ', np.mean(bert_r1))\n",
    "print('rouge2 average: ', np.mean(bert_r2))\n",
    "print('rougeL average: ', np.mean(bert_rL))\n",
    "print('rougeLs average:', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7408e182-dd09-4de7-8d0d-b7a349fe156d",
   "metadata": {},
   "source": [
    "**Category 4: entertainment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e690109b-3068-4607-92de-a3af82689194",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rouge1 average: ', np.mean(bert_r1))\n",
    "print('rouge2 average: ', np.mean(bert_r2))\n",
    "print('rougeL average: ', np.mean(bert_rL))\n",
    "print('rougeLs average:', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9728c1f3-c8c7-4a86-97cd-65c9ad25bbc9",
   "metadata": {},
   "source": [
    "**Category 5: technology**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bf940ada-ad3b-41ba-a68e-df45eef6b897",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average:  0.16956305823103446\n",
      "rouge2 average:  0.020127432450957147\n",
      "rougeL average:  0.11438383130539752\n",
      "rougeLs average: 0.11438383130539752\n",
      "chrf average: 26.085576408431866\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average: ', np.mean(bert_r1))\n",
    "print('rouge2 average: ', np.mean(bert_r2))\n",
    "print('rougeL average: ', np.mean(bert_rL))\n",
    "print('rougeLs average:', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
