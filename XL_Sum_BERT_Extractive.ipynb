{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d870553",
   "metadata": {},
   "source": [
    "**Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dacdaf3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/w266/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-03-27 21:44:52.996106: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-27 21:44:57.490615: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-03-27 21:44:57.490950: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-03-27 21:44:57.490961: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "#let's make longer output readable without horizontal scrolling\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f29e293",
   "metadata": {},
   "source": [
    "**Necessary Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc5f44ab-39ad-49be-a367-c2ff78ac845a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def get_default_args(func):\n",
    "    signature = inspect.signature(func)\n",
    "    return {\n",
    "        k: v.default\n",
    "        for k, v in signature.parameters.items()\n",
    "        if v.default is not inspect.Parameter.empty\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f96edd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586a6d00",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec6c665e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xlsum (/home/ubuntu/.cache/huggingface/datasets/csebuetnlp___xlsum/english/2.0.0/518ab0af76048660bcc2240ca6e8692a977c80e384ffb18fdddebaca6daebdce)\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  5.46it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"csebuetnlp/xlsum\", \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "111f7985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306522"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDA\n",
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f538fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'uk-scotland-highlands-islands-11069985',\n",
       " 'url': 'https://www.bbc.com/news/uk-scotland-highlands-islands-11069985',\n",
       " 'title': 'Huge tidal turbine installed at Orkney test site',\n",
       " 'summary': 'The massive tidal turbine AK1000 has been installed in 35m (114.8ft) of water at a test site in Orkney.',\n",
       " 'text': 'Atlantis Resources unveiled the marine energy device at Invergordon ahead of it being shipped to Kirkwall. Trials on the device will now be run at the European Marine Energy Centre test site off Eday. The device stands 22.5m (73ft) tall, weighs 1,300 tonnes and has two sets of blades on a single unit. It could generate enough power for 1,000 homes.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDA\n",
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "296f3f54-85c7-4446-ae73-fa54ac3123a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235420</th>\n",
       "      <td>235420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172024</th>\n",
       "      <td>172024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253546</th>\n",
       "      <td>253546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224954</th>\n",
       "      <td>224954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214134</th>\n",
       "      <td>214134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index\n",
       "235420  235420\n",
       "172024  172024\n",
       "253546  253546\n",
       "224954  224954\n",
       "214134  214134"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pd.DataFrame({\"index\": list(range(len(dataset['train'])))})\n",
    "sample_index = index.sample(n=2000, replace=False, random_state=1004)\n",
    "sample_index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6656d7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id = []\n",
    "url = []\n",
    "title = []\n",
    "article = []\n",
    "article_num_sentences = []\n",
    "article_num_characters = []\n",
    "summary = []\n",
    "summary_num_sentences = []\n",
    "summary_num_characters = []\n",
    "\n",
    "for i in sample_index[\"index\"]:\n",
    "    id.append(dataset[\"train\"][i]['id'])\n",
    "    url.append(dataset[\"train\"][i]['url'])\n",
    "    title.append(dataset[\"train\"][i]['title'])\n",
    "    summary.append(dataset[\"train\"][i]['summary'])\n",
    "    summary_num_sentences.append(len(dataset[\"train\"][i]['summary'].split(\".\")))\n",
    "    summary_num_characters.append(len(dataset[\"train\"][i]['summary']))\n",
    "    article.append(dataset[\"train\"][i]['text'])\n",
    "    article_num_sentences.append(len(dataset[\"train\"][i]['text'].split(\".\")))\n",
    "    article_num_characters.append(len(dataset[\"train\"][i]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66def3a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>article_num_sentences</th>\n",
       "      <th>article_num_characters</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_num_sentences</th>\n",
       "      <th>summary_num_characters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uk-england-cornwall-55191422</td>\n",
       "      <td>https://www.bbc.com/news/uk-england-cornwall-5...</td>\n",
       "      <td>Care home manager: 'It felt like we were losin...</td>\n",
       "      <td>By Rebecca Ricks &amp; Johnny O'SheaBBC Spotlight ...</td>\n",
       "      <td>37</td>\n",
       "      <td>3755</td>\n",
       "      <td>During the spring, at the height of the Covid-...</td>\n",
       "      <td>2</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uk-43893709</td>\n",
       "      <td>https://www.bbc.com/news/uk-43893709</td>\n",
       "      <td>Tafida Raqeeb: Who decides the care of sick ch...</td>\n",
       "      <td>By Rachel SchraerBBC Reality Check So, why did...</td>\n",
       "      <td>33</td>\n",
       "      <td>4531</td>\n",
       "      <td>The parents of five-year-old Tafida Raqeeb, wh...</td>\n",
       "      <td>2</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uk-politics-57050659</td>\n",
       "      <td>https://www.bbc.com/news/uk-politics-57050659</td>\n",
       "      <td>Labour reshuffle: Anneliese Dodds out in Starm...</td>\n",
       "      <td>Anneliese Dodds will now become the Labour Par...</td>\n",
       "      <td>36</td>\n",
       "      <td>4845</td>\n",
       "      <td>Sir Keir Starmer has sacked his shadow chancel...</td>\n",
       "      <td>2</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entertainment-arts-38221420</td>\n",
       "      <td>https://www.bbc.com/news/entertainment-arts-38...</td>\n",
       "      <td>Vinyl sales made more than downloads last week</td>\n",
       "      <td>By Mark SavageBBC Music reporter Vinyl sales m...</td>\n",
       "      <td>27</td>\n",
       "      <td>2082</td>\n",
       "      <td>More money was spent on vinyl than downloaded ...</td>\n",
       "      <td>2</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment-arts-24046991</td>\n",
       "      <td>https://www.bbc.com/news/entertainment-arts-24...</td>\n",
       "      <td>Pirates of the Caribbean sequel delayed</td>\n",
       "      <td>Disney's Pirates of The Caribbean: Dead Men Te...</td>\n",
       "      <td>14</td>\n",
       "      <td>1569</td>\n",
       "      <td>The next Pirates of the Caribbean film has bee...</td>\n",
       "      <td>2</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  \\\n",
       "0  uk-england-cornwall-55191422   \n",
       "1                   uk-43893709   \n",
       "2          uk-politics-57050659   \n",
       "3   entertainment-arts-38221420   \n",
       "4   entertainment-arts-24046991   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.bbc.com/news/uk-england-cornwall-5...   \n",
       "1               https://www.bbc.com/news/uk-43893709   \n",
       "2      https://www.bbc.com/news/uk-politics-57050659   \n",
       "3  https://www.bbc.com/news/entertainment-arts-38...   \n",
       "4  https://www.bbc.com/news/entertainment-arts-24...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Care home manager: 'It felt like we were losin...   \n",
       "1  Tafida Raqeeb: Who decides the care of sick ch...   \n",
       "2  Labour reshuffle: Anneliese Dodds out in Starm...   \n",
       "3     Vinyl sales made more than downloads last week   \n",
       "4            Pirates of the Caribbean sequel delayed   \n",
       "\n",
       "                                             article  article_num_sentences  \\\n",
       "0  By Rebecca Ricks & Johnny O'SheaBBC Spotlight ...                     37   \n",
       "1  By Rachel SchraerBBC Reality Check So, why did...                     33   \n",
       "2  Anneliese Dodds will now become the Labour Par...                     36   \n",
       "3  By Mark SavageBBC Music reporter Vinyl sales m...                     27   \n",
       "4  Disney's Pirates of The Caribbean: Dead Men Te...                     14   \n",
       "\n",
       "   article_num_characters                                            summary  \\\n",
       "0                    3755  During the spring, at the height of the Covid-...   \n",
       "1                    4531  The parents of five-year-old Tafida Raqeeb, wh...   \n",
       "2                    4845  Sir Keir Starmer has sacked his shadow chancel...   \n",
       "3                    2082  More money was spent on vinyl than downloaded ...   \n",
       "4                    1569  The next Pirates of the Caribbean film has bee...   \n",
       "\n",
       "   summary_num_sentences  summary_num_characters  \n",
       "0                      2                     147  \n",
       "1                      2                     121  \n",
       "2                      2                     115  \n",
       "3                      2                      83  \n",
       "4                      2                      88  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'id': id, 'url': url, \"title\": title, 'article': article, \"article_num_sentences\": article_num_sentences, \"article_num_characters\": article_num_characters, 'summary': summary,\"summary_num_sentences\": summary_num_sentences, \"summary_num_characters\": summary_num_characters}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4266569",
   "metadata": {},
   "source": [
    "**BERT Extractive Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c76d1-50eb-4560-b78c-3f05e2c0fe3e",
   "metadata": {},
   "source": [
    "Model 0: Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89fd59b8-44ed-4497-9f64-f0d14abbdecd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af9232aa-5fa4-4beb-a11a-e25092a1cdda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "\n",
    "for i in range(int(len(df['article'])/2)):\n",
    "    \n",
    "    candidate = model(df['article'][i],\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_0_scores.csv', index=False)\n",
    "        print(i)\n",
    "\n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_0_scores.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3230aee-fe28-4099-ae35-b9fd18f5d2f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.1555884135260613\n",
      "rouge2 average : 0.02396541867504151\n",
      "rougeL average : 0.1019488717714199\n",
      "rougeLs average : 0.1019488717714199\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846d219-5c6b-47b6-ae81-9a61e241ac74",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finetuning On First 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d9ce16-2e35-453d-8c3a-f1e1d354c68c",
   "metadata": {},
   "source": [
    "Model 1. Adjusted min_length, max_length, num_sentences (which overrides ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79dde227-1016-4f35-a0cd-b6a2f3d5f587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from summarizer import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8edfe31-ebe3-4c38-b8ef-7df7a013d1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'bert-large-uncased',\n",
       " 'custom_model': None,\n",
       " 'custom_tokenizer': None,\n",
       " 'hidden': -2,\n",
       " 'reduce_option': 'mean',\n",
       " 'sentence_handler': <summarizer.sentence_handler.SentenceHandler at 0x7ff4f591a2e0>,\n",
       " 'random_state': 12345,\n",
       " 'hidden_concat': False}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_default_args(Summarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "462afc0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using bert-base instead of bert-large to reduce run times\n",
    "model = Summarizer(model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8c09bea-3299-40b8-a6a2-a7a38c9c0c26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ratio': 0.2,\n",
       " 'min_length': 40,\n",
       " 'max_length': 600,\n",
       " 'use_first': True,\n",
       " 'algorithm': 'kmeans',\n",
       " 'num_sentences': None,\n",
       " 'return_as_list': False}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_default_args(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2595b5cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "\n",
    "for i in range(int(len(df['article'])/2)):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = round(df[\"summary_num_sentences\"][:1000].mean()), \n",
    "                      min_length = min(df[\"summary_num_characters\"][:1000]),\n",
    "                      max_length = max(df[\"summary_num_characters\"][:1000]),        \n",
    "                      ratio = None,\n",
    "                      use_first = None,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_1_scores.csv', index=False)\n",
    "        print(i)\n",
    "        \n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_1_scores.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "97555fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.178192403298457\n",
      "rouge2 average : 0.026733936113222955\n",
      "rougeL average : 0.11966303460985027\n",
      "rougeLs average : 0.11966303460985027\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec631b8-e0a8-40ac-8f16-fe3094b118cc",
   "metadata": {},
   "source": [
    "Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96727286-a2e4-4e08-a014-1d205a55dc44",
   "metadata": {},
   "source": [
    "2. Made num_sentences based off number of clusters instead of average "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68b71359-02c7-4236-b22e-5eb077d02a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 21:45:40.058167: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-03-27 21:45:40.058228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-55-240): /proc/driver/nvidia/version does not exist\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer(model='bert-base-uncased',\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e007312b-1348-45d4-ab1b-f32c270bb415",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "\n",
    "for i in range(int(len(df['article'])/2)):\n",
    "\n",
    "    res = model.calculate_optimal_k(df['article'][i], k_max=10)\n",
    "    \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = res, # number of sentences determined by number of clusters\n",
    "                      min_length = min(df[\"summary_num_characters\"][:1000]),\n",
    "                      max_length = max(df[\"summary_num_characters\"][:1000]),        \n",
    "                      ratio = None,\n",
    "                      use_first = None,)\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_2_scores.csv', index=False)\n",
    "        print(i)\n",
    "        \n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_2_scores.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "701447ab-000a-4aea-865a-c5244b0f342c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.16466988117522663\n",
      "rouge2 average : 0.026490759507054564\n",
      "rougeL average : 0.1089025377167142\n",
      "rougeLs average : 0.1089025377167142\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd8b62-7d3b-4a03-af4c-c9ecec023faa",
   "metadata": {},
   "source": [
    "3. Setting number of sentences with clusters didn't help in second model, so we are reverting back to first model but setting use_first = False bc many examples have meta data in first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b74b6df2-673c-4083-9939-6e30b722df4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from summarizer import Summarizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model = Summarizer(model='bert-base-uncased',\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "815245f2-2673-4ac4-a2c3-4085e5b920aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "\n",
    "for i in range(int(len(df['article'])/2)):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = round(df[\"summary_num_sentences\"][:1000].mean()), \n",
    "                      min_length = min(df[\"summary_num_characters\"][:1000]),\n",
    "                      max_length = max(df[\"summary_num_characters\"][:1000]),        \n",
    "                      ratio = None,\n",
    "                      use_first = False,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_3_scores.csv', index=False)\n",
    "        print(i)\n",
    "\n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_3_scores.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29baf035-e761-40da-abc0-9dd6ac2de9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.178192403298457\n",
      "rouge2 average : 0.026733936113222955\n",
      "rougeL average : 0.11966303460985027\n",
      "rougeLs average : 0.11966303460985027\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1715530b-d2c9-45bb-9152-423e3acc7394",
   "metadata": {},
   "source": [
    "4. Setting use_first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce1bd98a-1738-4785-8d76-d463896cfc53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer(model='bert-base-uncased',\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72ef153b-ad2e-4059-b24a-0dd2db495bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "\n",
    "for i in range(int(len(df['article'])/2)):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = round(df[\"summary_num_sentences\"][:1000].mean()), \n",
    "                      min_length = min(df[\"summary_num_characters\"][:1000]),\n",
    "                      max_length = max(df[\"summary_num_characters\"][:1000]),        \n",
    "                      ratio = None,\n",
    "                      use_first = True,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_3_scores.csv', index=False)\n",
    "        print(i)\n",
    "\n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_3_scores.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "082a64a6-0e87-4e36-8b47-b65f4942e139",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.1711426991129008\n",
      "rouge2 average : 0.026139674733637642\n",
      "rougeL average : 0.11264718864577299\n",
      "rougeLs average : 0.11264718864577299\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a566379-110d-4741-8439-1738fa5e3389",
   "metadata": {},
   "source": [
    "5. Use bert-large-uncased w/ best from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87a140b7-6255-4b2a-93e8-fbb10dcf912d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████| 571/571 [00:00<00:00, 225kB/s]\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Downloading pytorch_model.bin: 100%|████████| 1.34G/1.34G [00:03<00:00, 363MB/s]\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "Downloading (…)solve/main/vocab.txt: 100%|███| 232k/232k [00:00<00:00, 39.0MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|███| 28.0/28.0 [00:00<00:00, 11.5kB/s]\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer(model='bert-large-uncased',\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2934abbd-5dae-4f94-8e90-dc2e45a6fe6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "\n",
    "for i in range(int(len(df['article'])/2)):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = round(df[\"summary_num_sentences\"][:1000].mean()), \n",
    "                      min_length = min(df[\"summary_num_characters\"][:1000]),\n",
    "                      max_length = max(df[\"summary_num_characters\"][:1000]),        \n",
    "                      ratio = None,\n",
    "                      use_first = False,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_4_scores.csv', index=False)\n",
    "        print(i)\n",
    "\n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_4_scores.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7192f7f4-028a-4abc-aa1a-925a1a8c3616",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.17709196861695112\n",
      "rouge2 average : 0.025145083935109475\n",
      "rougeL average : 0.11939202129428797\n",
      "rougeLs average : 0.11939202129428797\n"
     ]
    }
   ],
   "source": [
    "# this model took longer and performed worse\n",
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cab172-dc0a-4962-990e-cefbec9f1366",
   "metadata": {},
   "source": [
    "6. Reduce option median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e5fa0c9-a090-4dff-b250-459de6983da5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer(model='bert-base-uncased', reduce_option = 'median'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cad742a-6f5b-4add-b9b3-a6ab0c856630",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "\n",
    "for i in range(int(len(df['article'])/2)):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = round(df[\"summary_num_sentences\"][:1000].mean()), \n",
    "                      min_length = min(df[\"summary_num_characters\"][:1000]),\n",
    "                      max_length = max(df[\"summary_num_characters\"][:1000]),        \n",
    "                      ratio = None,\n",
    "                      use_first = None,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_4_scores.csv', index=False)\n",
    "        print(i)\n",
    "\n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_6_scores.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81108fe5-f4f9-4509-ade6-75b9cb09815a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.17604122934873215\n",
      "rouge2 average : 0.025439640514237527\n",
      "rougeL average : 0.11815625686666714\n",
      "rougeLs average : 0.11815625686666714\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd379e9-9b75-49c2-a336-f132b63281e1",
   "metadata": {},
   "source": [
    "7. Reduce option max THIS WORKED THIS BEST!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "382d244f-a422-49b0-b141-0a7b55c4c163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer(model='bert-base-uncased', reduce_option = 'max'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f92f9a41-6fd4-4320-b51c-e026c01e01a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "\n",
    "for i in range(int(len(df['article'])/2)):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = round(df[\"summary_num_sentences\"][:1000].mean()), \n",
    "                      min_length = min(df[\"summary_num_characters\"][:1000]),\n",
    "                      max_length = max(df[\"summary_num_characters\"][:1000]),        \n",
    "                      ratio = None,\n",
    "                      use_first = None,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_7_scores.csv', index=False)\n",
    "        print(i)\n",
    "\n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_7_scores.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95cd8b50-8555-4b77-9a56-edc169ccc0eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.18111656560761913\n",
      "rouge2 average : 0.025095740456991585\n",
      "rougeL average : 0.12307647067888493\n",
      "rougeLs average : 0.12307647067888493\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
