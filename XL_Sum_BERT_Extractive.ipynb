{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d870553",
   "metadata": {},
   "source": [
    "**Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dacdaf3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/w266/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-04-01 07:12:51.217992: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-01 07:12:52.070613: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-04-01 07:12:52.070755: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-04-01 07:12:52.070767: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "#let's make longer output readable without horizontal scrolling\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f29e293",
   "metadata": {},
   "source": [
    "**Necessary Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc5f44ab-39ad-49be-a367-c2ff78ac845a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def get_default_args(func):\n",
    "    signature = inspect.signature(func)\n",
    "    return {\n",
    "        k: v.default\n",
    "        for k, v in signature.parameters.items()\n",
    "        if v.default is not inspect.Parameter.empty\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f96edd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e922d01-d029-42ee-bf38-6765115f1e04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chrf = evaluate.load(\"chrf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586a6d00",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec6c665e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xlsum (/home/ubuntu/.cache/huggingface/datasets/csebuetnlp___xlsum/english/2.0.0/518ab0af76048660bcc2240ca6e8692a977c80e384ffb18fdddebaca6daebdce)\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 339.65it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"csebuetnlp/xlsum\", \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "296f3f54-85c7-4446-ae73-fa54ac3123a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235420</th>\n",
       "      <td>235420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172024</th>\n",
       "      <td>172024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253546</th>\n",
       "      <td>253546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224954</th>\n",
       "      <td>224954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214134</th>\n",
       "      <td>214134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index\n",
       "235420  235420\n",
       "172024  172024\n",
       "253546  253546\n",
       "224954  224954\n",
       "214134  214134"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pd.DataFrame({\"index\": list(range(len(dataset['train'])))})\n",
    "sample_index = index.sample(n=2000, replace=False, random_state=1004)\n",
    "sample_index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6656d7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id = []\n",
    "url = []\n",
    "title = []\n",
    "article = []\n",
    "article_num_sentences = []\n",
    "article_num_characters = []\n",
    "summary = []\n",
    "summary_num_sentences = []\n",
    "summary_num_characters = []\n",
    "\n",
    "for i in sample_index[\"index\"]:\n",
    "    id.append(dataset[\"train\"][i]['id'])\n",
    "    url.append(dataset[\"train\"][i]['url'])\n",
    "    title.append(dataset[\"train\"][i]['title'])\n",
    "    summary.append(dataset[\"train\"][i]['summary'])\n",
    "    summary_num_sentences.append(len(dataset[\"train\"][i]['summary'].split(\".\")))\n",
    "    summary_num_characters.append(len(dataset[\"train\"][i]['summary']))\n",
    "    article.append(dataset[\"train\"][i]['text'])\n",
    "    article_num_sentences.append(len(dataset[\"train\"][i]['text'].split(\".\")))\n",
    "    article_num_characters.append(len(dataset[\"train\"][i]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66def3a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>article_num_sentences</th>\n",
       "      <th>article_num_characters</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_num_sentences</th>\n",
       "      <th>summary_num_characters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uk-england-cornwall-55191422</td>\n",
       "      <td>https://www.bbc.com/news/uk-england-cornwall-5...</td>\n",
       "      <td>Care home manager: 'It felt like we were losin...</td>\n",
       "      <td>By Rebecca Ricks &amp; Johnny O'SheaBBC Spotlight ...</td>\n",
       "      <td>37</td>\n",
       "      <td>3755</td>\n",
       "      <td>During the spring, at the height of the Covid-...</td>\n",
       "      <td>2</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uk-43893709</td>\n",
       "      <td>https://www.bbc.com/news/uk-43893709</td>\n",
       "      <td>Tafida Raqeeb: Who decides the care of sick ch...</td>\n",
       "      <td>By Rachel SchraerBBC Reality Check So, why did...</td>\n",
       "      <td>33</td>\n",
       "      <td>4531</td>\n",
       "      <td>The parents of five-year-old Tafida Raqeeb, wh...</td>\n",
       "      <td>2</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uk-politics-57050659</td>\n",
       "      <td>https://www.bbc.com/news/uk-politics-57050659</td>\n",
       "      <td>Labour reshuffle: Anneliese Dodds out in Starm...</td>\n",
       "      <td>Anneliese Dodds will now become the Labour Par...</td>\n",
       "      <td>36</td>\n",
       "      <td>4845</td>\n",
       "      <td>Sir Keir Starmer has sacked his shadow chancel...</td>\n",
       "      <td>2</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entertainment-arts-38221420</td>\n",
       "      <td>https://www.bbc.com/news/entertainment-arts-38...</td>\n",
       "      <td>Vinyl sales made more than downloads last week</td>\n",
       "      <td>By Mark SavageBBC Music reporter Vinyl sales m...</td>\n",
       "      <td>27</td>\n",
       "      <td>2082</td>\n",
       "      <td>More money was spent on vinyl than downloaded ...</td>\n",
       "      <td>2</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment-arts-24046991</td>\n",
       "      <td>https://www.bbc.com/news/entertainment-arts-24...</td>\n",
       "      <td>Pirates of the Caribbean sequel delayed</td>\n",
       "      <td>Disney's Pirates of The Caribbean: Dead Men Te...</td>\n",
       "      <td>14</td>\n",
       "      <td>1569</td>\n",
       "      <td>The next Pirates of the Caribbean film has bee...</td>\n",
       "      <td>2</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  \\\n",
       "0  uk-england-cornwall-55191422   \n",
       "1                   uk-43893709   \n",
       "2          uk-politics-57050659   \n",
       "3   entertainment-arts-38221420   \n",
       "4   entertainment-arts-24046991   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.bbc.com/news/uk-england-cornwall-5...   \n",
       "1               https://www.bbc.com/news/uk-43893709   \n",
       "2      https://www.bbc.com/news/uk-politics-57050659   \n",
       "3  https://www.bbc.com/news/entertainment-arts-38...   \n",
       "4  https://www.bbc.com/news/entertainment-arts-24...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Care home manager: 'It felt like we were losin...   \n",
       "1  Tafida Raqeeb: Who decides the care of sick ch...   \n",
       "2  Labour reshuffle: Anneliese Dodds out in Starm...   \n",
       "3     Vinyl sales made more than downloads last week   \n",
       "4            Pirates of the Caribbean sequel delayed   \n",
       "\n",
       "                                             article  article_num_sentences  \\\n",
       "0  By Rebecca Ricks & Johnny O'SheaBBC Spotlight ...                     37   \n",
       "1  By Rachel SchraerBBC Reality Check So, why did...                     33   \n",
       "2  Anneliese Dodds will now become the Labour Par...                     36   \n",
       "3  By Mark SavageBBC Music reporter Vinyl sales m...                     27   \n",
       "4  Disney's Pirates of The Caribbean: Dead Men Te...                     14   \n",
       "\n",
       "   article_num_characters                                            summary  \\\n",
       "0                    3755  During the spring, at the height of the Covid-...   \n",
       "1                    4531  The parents of five-year-old Tafida Raqeeb, wh...   \n",
       "2                    4845  Sir Keir Starmer has sacked his shadow chancel...   \n",
       "3                    2082  More money was spent on vinyl than downloaded ...   \n",
       "4                    1569  The next Pirates of the Caribbean film has bee...   \n",
       "\n",
       "   summary_num_sentences  summary_num_characters  \n",
       "0                      2                     147  \n",
       "1                      2                     121  \n",
       "2                      2                     115  \n",
       "3                      2                      83  \n",
       "4                      2                      88  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'id': id, 'url': url, \"title\": title, 'article': article, \"article_num_sentences\": article_num_sentences, \"article_num_characters\": article_num_characters, 'summary': summary,\"summary_num_sentences\": summary_num_sentences, \"summary_num_characters\": summary_num_characters}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f872e-a7c8-4058-8ba0-7935db2207b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Default Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a312a65-1c63-4251-8553-c66959b057a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 07:12:58.971816: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-04-01 07:12:58.971873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-55-240): /proc/driver/nvidia/version does not exist\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8edfe31-ebe3-4c38-b8ef-7df7a013d1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'bert-large-uncased',\n",
       " 'custom_model': None,\n",
       " 'custom_tokenizer': None,\n",
       " 'hidden': -2,\n",
       " 'reduce_option': 'mean',\n",
       " 'sentence_handler': <summarizer.sentence_handler.SentenceHandler at 0x7f694e91d8b0>,\n",
       " 'random_state': 12345,\n",
       " 'hidden_concat': False}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_default_args(Summarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8c09bea-3299-40b8-a6a2-a7a38c9c0c26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ratio': 0.2,\n",
       " 'min_length': 40,\n",
       " 'max_length': 600,\n",
       " 'use_first': True,\n",
       " 'algorithm': 'kmeans',\n",
       " 'num_sentences': None,\n",
       " 'return_as_list': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_default_args(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4266569",
   "metadata": {},
   "source": [
    "**BERT Extractive Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c76d1-50eb-4560-b78c-3f05e2c0fe3e",
   "metadata": {},
   "source": [
    "Model 0: Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89fd59b8-44ed-4497-9f64-f0d14abbdecd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af9232aa-5fa4-4beb-a11a-e25092a1cdda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(int(len(df['article'])/2)):\n",
    "    \n",
    "    candidate = model(df['article'][i],\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_0_scores.csv', index=False)\n",
    "        print(i)\n",
    "\n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_0_scores.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3230aee-fe28-4099-ae35-b9fd18f5d2f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.1555884135260613\n",
      "rouge2 average : 0.02396541867504151\n",
      "rougeL average : 0.1019488717714199\n",
      "rougeLs average : 0.1019488717714199\n",
      "chrf average: 24.023480987952347\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846d219-5c6b-47b6-ae81-9a61e241ac74",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finetuning On First 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d9ce16-2e35-453d-8c3a-f1e1d354c68c",
   "metadata": {},
   "source": [
    "Model 1. Adjusted min_length, max_length, num_sentences (which overrides ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79dde227-1016-4f35-a0cd-b6a2f3d5f587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from summarizer import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "462afc0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using bert-base instead of bert-large to reduce run times\n",
    "model = Summarizer(model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2595b5cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(int(len(df['article'])/2)):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = round(df[\"summary_num_sentences\"][:1000].mean()), \n",
    "                      min_length = min(df[\"summary_num_characters\"][:1000]),\n",
    "                      max_length = max(df[\"summary_num_characters\"][:1000]),        \n",
    "                      ratio = None,\n",
    "                      use_first = None,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_1_scores.csv', index=False)\n",
    "        print(i)\n",
    "        \n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_1_scores.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97555fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.178192403298457\n",
      "rouge2 average : 0.026733936113222955\n",
      "rougeL average : 0.11966303460985027\n",
      "rougeLs average : 0.11966303460985027\n",
      "chrf average: 26.369458875111786\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec631b8-e0a8-40ac-8f16-fe3094b118cc",
   "metadata": {},
   "source": [
    "Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96727286-a2e4-4e08-a014-1d205a55dc44",
   "metadata": {},
   "source": [
    "2. Made num_sentences based off number of clusters instead of average "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68b71359-02c7-4236-b22e-5eb077d02a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer(model='bert-base-uncased',\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e007312b-1348-45d4-ab1b-f32c270bb415",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(int(len(df['article'])/2)):\n",
    "\n",
    "    res = model.calculate_optimal_k(df['article'][i], k_max=10)\n",
    "    \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = res, # number of sentences determined by number of clusters\n",
    "                      min_length = min(df[\"summary_num_characters\"][:1000]),\n",
    "                      max_length = max(df[\"summary_num_characters\"][:1000]),        \n",
    "                      ratio = None,\n",
    "                      use_first = None,)\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_2_scores.csv', index=False)\n",
    "        print(i)\n",
    "        \n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_2_scores.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "701447ab-000a-4aea-865a-c5244b0f342c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.16466988117522663\n",
      "rouge2 average : 0.026490759507054564\n",
      "rougeL average : 0.1089025377167142\n",
      "rougeLs average : 0.1089025377167142\n",
      "chrf average: 25.491807322933525\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd8b62-7d3b-4a03-af4c-c9ecec023faa",
   "metadata": {},
   "source": [
    "3. Setting number of sentences with clusters didn't help in second model, so we are reverting back to first model but setting use_first = False bc many examples have meta data in first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b74b6df2-673c-4083-9939-6e30b722df4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from summarizer import Summarizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model = Summarizer(model='bert-base-uncased',\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "815245f2-2673-4ac4-a2c3-4085e5b920aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(int(len(df['article'])/2)):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = round(df[\"summary_num_sentences\"][:1000].mean()), \n",
    "                      min_length = min(df[\"summary_num_characters\"][:1000]),\n",
    "                      max_length = max(df[\"summary_num_characters\"][:1000]),        \n",
    "                      ratio = None,\n",
    "                      use_first = False,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "                            \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_3_scores.csv', index=False)\n",
    "        print(i)\n",
    "\n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_3_scores.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29baf035-e761-40da-abc0-9dd6ac2de9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.178192403298457\n",
      "rouge2 average : 0.026733936113222955\n",
      "rougeL average : 0.11966303460985027\n",
      "rougeLs average : 0.11966303460985027\n",
      "chrf average: 26.369458875111786\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1715530b-d2c9-45bb-9152-423e3acc7394",
   "metadata": {},
   "source": [
    "4. Setting use_first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce1bd98a-1738-4785-8d76-d463896cfc53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer(model='bert-base-uncased',\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72ef153b-ad2e-4059-b24a-0dd2db495bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(int(len(df['article'])/2)):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = round(df[\"summary_num_sentences\"][:1000].mean()), \n",
    "                      min_length = min(df[\"summary_num_characters\"][:1000]),\n",
    "                      max_length = max(df[\"summary_num_characters\"][:1000]),        \n",
    "                      ratio = None,\n",
    "                      use_first = True,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_3_scores.csv', index=False)\n",
    "        print(i)\n",
    "\n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_3_scores.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "082a64a6-0e87-4e36-8b47-b65f4942e139",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.1711426991129008\n",
      "rouge2 average : 0.026139674733637642\n",
      "rougeL average : 0.11264718864577299\n",
      "rougeLs average : 0.11264718864577299\n",
      "chrf average: 25.747682911450475\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a566379-110d-4741-8439-1738fa5e3389",
   "metadata": {},
   "source": [
    "5. Use bert-large-uncased w/ best from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87a140b7-6255-4b2a-93e8-fbb10dcf912d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer(model='bert-large-uncased',\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2934abbd-5dae-4f94-8e90-dc2e45a6fe6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(int(len(df['article'])/2)):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = round(df[\"summary_num_sentences\"][:1000].mean()), \n",
    "                      min_length = min(df[\"summary_num_characters\"][:1000]),\n",
    "                      max_length = max(df[\"summary_num_characters\"][:1000]),        \n",
    "                      ratio = None,\n",
    "                      use_first = False,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_4_scores.csv', index=False)\n",
    "        print(i)\n",
    "\n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_4_scores.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7192f7f4-028a-4abc-aa1a-925a1a8c3616",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.17709196861695112\n",
      "rouge2 average : 0.025145083935109475\n",
      "rougeL average : 0.11939202129428797\n",
      "rougeLs average : 0.11939202129428797\n",
      "chrf average: 26.173252280733426\n"
     ]
    }
   ],
   "source": [
    "# this model took longer and performed worse\n",
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cab172-dc0a-4962-990e-cefbec9f1366",
   "metadata": {},
   "source": [
    "6. Reduce option median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e5fa0c9-a090-4dff-b250-459de6983da5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer(model='bert-base-uncased', reduce_option = 'median'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9cad742a-6f5b-4add-b9b3-a6ab0c856630",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(int(len(df['article'])/2)):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = round(df[\"summary_num_sentences\"][:1000].mean()), \n",
    "                      min_length = min(df[\"summary_num_characters\"][:1000]),\n",
    "                      max_length = max(df[\"summary_num_characters\"][:1000]),        \n",
    "                      ratio = None,\n",
    "                      use_first = None,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_4_scores.csv', index=False)\n",
    "        print(i)\n",
    "\n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_6_scores.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81108fe5-f4f9-4509-ade6-75b9cb09815a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.17604122934873215\n",
      "rouge2 average : 0.025439640514237527\n",
      "rougeL average : 0.11815625686666714\n",
      "rougeLs average : 0.11815625686666714\n",
      "chrf average: 26.217912847303975\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd379e9-9b75-49c2-a336-f132b63281e1",
   "metadata": {},
   "source": [
    "**7. Reduce option max THIS WORKED THIS BEST!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "382d244f-a422-49b0-b141-0a7b55c4c163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer(model='bert-base-uncased', reduce_option = 'max'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f92f9a41-6fd4-4320-b51c-e026c01e01a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(int(len(df['article'])/2)):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = round(df[\"summary_num_sentences\"][:1000].mean()), \n",
    "                      min_length = min(df[\"summary_num_characters\"][:1000]),\n",
    "                      max_length = max(df[\"summary_num_characters\"][:1000]),        \n",
    "                      ratio = None,\n",
    "                      use_first = None,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_7_scores.csv', index=False)\n",
    "        print(i)\n",
    "\n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_7_scores.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95cd8b50-8555-4b77-9a56-edc169ccc0eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average : 0.18111656560761913\n",
      "rouge2 average : 0.025095740456991585\n",
      "rougeL average : 0.12307647067888493\n",
      "rougeLs average : 0.12307647067888493\n",
      "chrf average: 26.458573826280013\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average :', np.mean(bert_r1))\n",
    "print('rouge2 average :', np.mean(bert_r2))\n",
    "print('rougeL average :', np.mean(bert_rL))\n",
    "print('rougeLs average :', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50baf0e3-b028-4533-bc6c-7be20529119e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_sentences 2\n",
      "min_length 45\n",
      "max_length 497\n"
     ]
    }
   ],
   "source": [
    "print('num_sentences', round(df[\"summary_num_sentences\"][:1000].mean()))\n",
    "print('min_length', min(df[\"summary_num_characters\"][:1000]))\n",
    "print('max_length', max(df[\"summary_num_characters\"][:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fbb598-7393-4006-8886-707879a490e1",
   "metadata": {},
   "source": [
    "**Best BERT Extractive model based on ROUGE scores from finetuning above by category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b40eb831-0089-44bd-aeaf-441de4acf8c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_indices(list_to_check, item_to_find):\n",
    "    indices = []\n",
    "    for idx, value in enumerate(list_to_check):\n",
    "        if value == item_to_find:\n",
    "            indices.append(idx)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "593b516b-48f1-43fa-af11-997604ee9072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories = []\n",
    "\n",
    "for i in range(len(dataset['train'])):\n",
    "    cat = dataset['train'][i]['id']\n",
    "    result = re.sub('\\d','',cat)[:-1]\n",
    "    result = result.split('-')[0].split('.')[0]\n",
    "    categories.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05fdfde-349d-4598-b490-1cbd3fda6c75",
   "metadata": {},
   "source": [
    "**Category 1: uk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8837067b-6bcb-477c-9f31-c52038232b44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103144</th>\n",
       "      <td>184549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135979</th>\n",
       "      <td>242139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122918</th>\n",
       "      <td>219346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41576</th>\n",
       "      <td>76571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23230</th>\n",
       "      <td>44271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index\n",
       "103144  184549\n",
       "135979  242139\n",
       "122918  219346\n",
       "41576    76571\n",
       "23230    44271"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk = find_indices(categories, 'uk')\n",
    "index = pd.DataFrame({\"index\": uk})\n",
    "sample_index = index.sample(n=1000, replace=False, random_state=1004)\n",
    "sample_index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a64fecc1-9498-4a9c-821b-c4a02827b42a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id = []\n",
    "url = []\n",
    "title = []\n",
    "article = []\n",
    "summary = []\n",
    "\n",
    "for i in sample_index[\"index\"]:\n",
    "    id.append(dataset[\"train\"][i]['id'])\n",
    "    url.append(dataset[\"train\"][i]['url'])\n",
    "    title.append(dataset[\"train\"][i]['title'])\n",
    "    summary.append(dataset[\"train\"][i]['summary'])\n",
    "    article.append(dataset[\"train\"][i]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19da3f13-2c99-4d97-814f-682d2dda99db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uk-england-bristol-51032231</td>\n",
       "      <td>https://www.bbc.com/news/uk-england-bristol-51...</td>\n",
       "      <td>Author Emily Koch: 'I'm not angry at the drive...</td>\n",
       "      <td>Emily Koch suffered two broken legs and ligame...</td>\n",
       "      <td>A woman left seriously injured after being hit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uk-england-birmingham-54264699</td>\n",
       "      <td>https://www.bbc.com/news/uk-england-birmingham...</td>\n",
       "      <td>Blind TikTok star Lucy Edwards says reaction t...</td>\n",
       "      <td>Lucy Edwards, who became Radio 1's first blind...</td>\n",
       "      <td>A blind vlogger hopes her TikTok videos on liv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uk-england-london-16091997</td>\n",
       "      <td>https://www.bbc.com/news/uk-england-london-160...</td>\n",
       "      <td>Bendy bus makes final journey for Transport fo...</td>\n",
       "      <td>The vehicles were used on 12 routes over the p...</td>\n",
       "      <td>The last of London's bendy buses was taken off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uk-england-norfolk-54058083</td>\n",
       "      <td>https://www.bbc.com/news/uk-england-norfolk-54...</td>\n",
       "      <td>Horsey seals: Volunteers remove rubber ring fr...</td>\n",
       "      <td>Four members of Friends of Horsey Seals netted...</td>\n",
       "      <td>Volunteers have helped capture a \"feisty\" seal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uk-scotland-edinburgh-east-fife-28838287</td>\n",
       "      <td>https://www.bbc.com/news/uk-scotland-edinburgh...</td>\n",
       "      <td>Tim Vine wins funniest Edinburgh Fringe joke a...</td>\n",
       "      <td>He won with the one-liner: \"I decided to sell ...</td>\n",
       "      <td>A joke by comedian Tim Vine about a vacuum cle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0               uk-england-bristol-51032231   \n",
       "1            uk-england-birmingham-54264699   \n",
       "2                uk-england-london-16091997   \n",
       "3               uk-england-norfolk-54058083   \n",
       "4  uk-scotland-edinburgh-east-fife-28838287   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.bbc.com/news/uk-england-bristol-51...   \n",
       "1  https://www.bbc.com/news/uk-england-birmingham...   \n",
       "2  https://www.bbc.com/news/uk-england-london-160...   \n",
       "3  https://www.bbc.com/news/uk-england-norfolk-54...   \n",
       "4  https://www.bbc.com/news/uk-scotland-edinburgh...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Author Emily Koch: 'I'm not angry at the drive...   \n",
       "1  Blind TikTok star Lucy Edwards says reaction t...   \n",
       "2  Bendy bus makes final journey for Transport fo...   \n",
       "3  Horsey seals: Volunteers remove rubber ring fr...   \n",
       "4  Tim Vine wins funniest Edinburgh Fringe joke a...   \n",
       "\n",
       "                                             article  \\\n",
       "0  Emily Koch suffered two broken legs and ligame...   \n",
       "1  Lucy Edwards, who became Radio 1's first blind...   \n",
       "2  The vehicles were used on 12 routes over the p...   \n",
       "3  Four members of Friends of Horsey Seals netted...   \n",
       "4  He won with the one-liner: \"I decided to sell ...   \n",
       "\n",
       "                                             summary  \n",
       "0  A woman left seriously injured after being hit...  \n",
       "1  A blind vlogger hopes her TikTok videos on liv...  \n",
       "2  The last of London's bendy buses was taken off...  \n",
       "3  Volunteers have helped capture a \"feisty\" seal...  \n",
       "4  A joke by comedian Tim Vine about a vacuum cle...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'id': id, 'url': url, \"title\": title, 'article': article, 'summary': summary}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34265006-89d9-43b0-a9bf-8e12ffd174ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(int(len(df['article']))):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = 2, \n",
    "                      min_length = 45,\n",
    "                      max_length = 497,        \n",
    "                      ratio = None,\n",
    "                      use_first = None,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_7_scores_uk.csv', index=False)\n",
    "        print(i)\n",
    "\n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_7_scores_uk.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9549992c-d3d4-42d8-a3a2-57b774e36e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average:  0.1867497432269535\n",
      "rouge2 average:  0.029340134895117883\n",
      "rougeL average:  0.12861916655749933\n",
      "rougeLs average: 0.12861916655749933\n",
      "chrf average: 26.629013615609452\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average: ', np.mean(bert_r1))\n",
    "print('rouge2 average: ', np.mean(bert_r2))\n",
    "print('rougeL average: ', np.mean(bert_rL))\n",
    "print('rougeLs average:', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ff031d-c333-4c0e-a319-6cba1f8b5da6",
   "metadata": {},
   "source": [
    "**Category 2: world**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "edff48ba-f34a-401b-bf1a-e22ccf5f173e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38266</th>\n",
       "      <td>209050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>25805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14047</th>\n",
       "      <td>71645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>127936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25699</th>\n",
       "      <td>137681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index\n",
       "38266  209050\n",
       "5910    25805\n",
       "14047   71645\n",
       "23996  127936\n",
       "25699  137681"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_inx = find_indices(categories, 'world')\n",
    "index = pd.DataFrame({\"index\": cat_inx})\n",
    "sample_index = index.sample(n=1000, replace=False, random_state=1004)\n",
    "sample_index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b287371-9892-4a1f-84af-229c0037e7d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id = []\n",
    "url = []\n",
    "title = []\n",
    "article = []\n",
    "summary = []\n",
    "\n",
    "for i in sample_index[\"index\"]:\n",
    "    id.append(dataset[\"train\"][i]['id'])\n",
    "    url.append(dataset[\"train\"][i]['url'])\n",
    "    title.append(dataset[\"train\"][i]['title'])\n",
    "    summary.append(dataset[\"train\"][i]['summary'])\n",
    "    article.append(dataset[\"train\"][i]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff9d1553-d738-49a0-a63d-1e5ebc8db46e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>world-asia-53847400</td>\n",
       "      <td>https://www.bbc.com/news/world-asia-53847400</td>\n",
       "      <td>Kim Jong-un gives sister Yo-jong 'more respons...</td>\n",
       "      <td>Mr Kim still maintains \"absolute authority\", b...</td>\n",
       "      <td>North Korean leader Kim Jong-un has delegated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>world-europe-guernsey-18129181</td>\n",
       "      <td>https://www.bbc.com/news/world-europe-guernsey...</td>\n",
       "      <td>Guernsey overgrown trees prompt road crash fears</td>\n",
       "      <td>Overgrown bushes and trees could result in peo...</td>\n",
       "      <td>Residents are being urged to trim vegetation o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>world-africa-44629681</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-africa-44629681</td>\n",
       "      <td>Bringing Gay Pride to Africa's last absolute m...</td>\n",
       "      <td>And anyone doubting the determination needed t...</td>\n",
       "      <td>Africa's last absolute monarchy is holding its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>world-asia-41840069</td>\n",
       "      <td>https://www.bbc.com/news/world-asia-41840069</td>\n",
       "      <td>Pakistan polygamy: Lahore man jailed over unap...</td>\n",
       "      <td>The court in Lahore also ordered Shahzad Saqib...</td>\n",
       "      <td>A Pakistan court has jailed a man for six mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>world-europe-guernsey-13052445</td>\n",
       "      <td>https://www.bbc.com/news/world-europe-guernsey...</td>\n",
       "      <td>Alternative recycling bank site proposed</td>\n",
       "      <td>The old facility at Manor Stores closed last y...</td>\n",
       "      <td>Work is continuing to try to find a permanent ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id  \\\n",
       "0             world-asia-53847400   \n",
       "1  world-europe-guernsey-18129181   \n",
       "2           world-africa-44629681   \n",
       "3             world-asia-41840069   \n",
       "4  world-europe-guernsey-13052445   \n",
       "\n",
       "                                                 url  \\\n",
       "0       https://www.bbc.com/news/world-asia-53847400   \n",
       "1  https://www.bbc.com/news/world-europe-guernsey...   \n",
       "2   https://www.bbc.co.uk/news/world-africa-44629681   \n",
       "3       https://www.bbc.com/news/world-asia-41840069   \n",
       "4  https://www.bbc.com/news/world-europe-guernsey...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Kim Jong-un gives sister Yo-jong 'more respons...   \n",
       "1   Guernsey overgrown trees prompt road crash fears   \n",
       "2  Bringing Gay Pride to Africa's last absolute m...   \n",
       "3  Pakistan polygamy: Lahore man jailed over unap...   \n",
       "4           Alternative recycling bank site proposed   \n",
       "\n",
       "                                             article  \\\n",
       "0  Mr Kim still maintains \"absolute authority\", b...   \n",
       "1  Overgrown bushes and trees could result in peo...   \n",
       "2  And anyone doubting the determination needed t...   \n",
       "3  The court in Lahore also ordered Shahzad Saqib...   \n",
       "4  The old facility at Manor Stores closed last y...   \n",
       "\n",
       "                                             summary  \n",
       "0  North Korean leader Kim Jong-un has delegated ...  \n",
       "1  Residents are being urged to trim vegetation o...  \n",
       "2  Africa's last absolute monarchy is holding its...  \n",
       "3  A Pakistan court has jailed a man for six mont...  \n",
       "4  Work is continuing to try to find a permanent ...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'id': id, 'url': url, \"title\": title, 'article': article, 'summary': summary}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f322b098-139a-487e-9078-7ad6de108469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(int(len(df['article']))):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = 2, \n",
    "                      min_length = 45,\n",
    "                      max_length = 497,        \n",
    "                      ratio = None,\n",
    "                      use_first = None,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_7_scores_world.csv', index=False)\n",
    "        print(i)\n",
    "\n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_7_scores_world.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7bb5d47e-fd5a-4b36-a84b-cdce83404763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average:  0.18598478113951733\n",
      "rouge2 average:  0.024305168822414184\n",
      "rougeL average:  0.12399975937641648\n",
      "rougeLs average: 0.12399975937641648\n",
      "chrf average: 27.03394659264641\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average: ', np.mean(bert_r1))\n",
    "print('rouge2 average: ', np.mean(bert_r2))\n",
    "print('rougeL average: ', np.mean(bert_rL))\n",
    "print('rougeLs average:', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c3dcc-ecb5-434c-979f-3e60174819cf",
   "metadata": {},
   "source": [
    "**Category 3: business**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "278c4350-fda9-41af-b3df-01c05c05dfaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17751</th>\n",
       "      <td>250222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8987</th>\n",
       "      <td>132229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>46895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17482</th>\n",
       "      <td>246643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13624</th>\n",
       "      <td>194415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index\n",
       "17751  250222\n",
       "8987   132229\n",
       "2855    46895\n",
       "17482  246643\n",
       "13624  194415"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_inx = find_indices(categories, 'business')\n",
    "index = pd.DataFrame({\"index\": cat_inx})\n",
    "sample_index = index.sample(n=1000, replace=False, random_state=1004)\n",
    "sample_index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e819c047-8508-4c2b-8c75-66d6053b9e3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id = []\n",
    "url = []\n",
    "title = []\n",
    "article = []\n",
    "summary = []\n",
    "\n",
    "for i in sample_index[\"index\"]:\n",
    "    id.append(dataset[\"train\"][i]['id'])\n",
    "    url.append(dataset[\"train\"][i]['url'])\n",
    "    title.append(dataset[\"train\"][i]['title'])\n",
    "    summary.append(dataset[\"train\"][i]['summary'])\n",
    "    article.append(dataset[\"train\"][i]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc111fb4-f3ea-4441-a4b8-eda56fcb3dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business-51487191</td>\n",
       "      <td>https://www.bbc.co.uk/news/business-51487191</td>\n",
       "      <td>British Gas owner Centrica hit by energy price...</td>\n",
       "      <td>Centrica made a loss of £849m in the year ende...</td>\n",
       "      <td>British Gas owner Centrica has blamed a big lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business-18289184</td>\n",
       "      <td>https://www.bbc.com/news/business-18289184</td>\n",
       "      <td>Pakistan economy at a crossroads</td>\n",
       "      <td>By Shahzeb JillaniBBC News The good news is th...</td>\n",
       "      <td>Pakistan is a nuclear weapon state with a popu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business-48524850</td>\n",
       "      <td>https://www.bbc.com/news/business-48524850</td>\n",
       "      <td>Green secures rescue deal for Topshop empire</td>\n",
       "      <td>The plan, covering stores including Topshop, M...</td>\n",
       "      <td>Sir Philip Green's Arcadia retail empire has b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business-54004709</td>\n",
       "      <td>https://www.bbc.com/news/business-54004709</td>\n",
       "      <td>Coronavirus: 'I took a £1,000 flight to beat P...</td>\n",
       "      <td>By Simon ReadBusiness reporter But he had to s...</td>\n",
       "      <td>John Cushing is cutting his Portugal holiday s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business-37339981</td>\n",
       "      <td>https://www.bbc.com/news/business-37339981</td>\n",
       "      <td>Tax credits mum 'falsely accused of marriage t...</td>\n",
       "      <td>By Peter Whittlesea &amp; John OwenVictoria Derbys...</td>\n",
       "      <td>A teenage mother had her child tax credits sto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                           url  \\\n",
       "0  business-51487191  https://www.bbc.co.uk/news/business-51487191   \n",
       "1  business-18289184    https://www.bbc.com/news/business-18289184   \n",
       "2  business-48524850    https://www.bbc.com/news/business-48524850   \n",
       "3  business-54004709    https://www.bbc.com/news/business-54004709   \n",
       "4  business-37339981    https://www.bbc.com/news/business-37339981   \n",
       "\n",
       "                                               title  \\\n",
       "0  British Gas owner Centrica hit by energy price...   \n",
       "1                   Pakistan economy at a crossroads   \n",
       "2       Green secures rescue deal for Topshop empire   \n",
       "3  Coronavirus: 'I took a £1,000 flight to beat P...   \n",
       "4  Tax credits mum 'falsely accused of marriage t...   \n",
       "\n",
       "                                             article  \\\n",
       "0  Centrica made a loss of £849m in the year ende...   \n",
       "1  By Shahzeb JillaniBBC News The good news is th...   \n",
       "2  The plan, covering stores including Topshop, M...   \n",
       "3  By Simon ReadBusiness reporter But he had to s...   \n",
       "4  By Peter Whittlesea & John OwenVictoria Derbys...   \n",
       "\n",
       "                                             summary  \n",
       "0  British Gas owner Centrica has blamed a big lo...  \n",
       "1  Pakistan is a nuclear weapon state with a popu...  \n",
       "2  Sir Philip Green's Arcadia retail empire has b...  \n",
       "3  John Cushing is cutting his Portugal holiday s...  \n",
       "4  A teenage mother had her child tax credits sto...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'id': id, 'url': url, \"title\": title, 'article': article, 'summary': summary}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2d219fd4-ba87-4828-bf37-407f78ec44e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(int(len(df['article']))):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = 2, \n",
    "                      min_length = 45,\n",
    "                      max_length = 497,         \n",
    "                      ratio = None,\n",
    "                      use_first = None,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_7_scores_business.csv', index=False)\n",
    "        print(i)\n",
    "\n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_7_scores_business.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "56801ea6-6e55-41e7-b984-c38c67221ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average:  0.182531265620785\n",
      "rouge2 average:  0.025272652711743917\n",
      "rougeL average:  0.12445405259295919\n",
      "rougeLs average: 0.12445405259295919\n",
      "chrf average: 26.372235104614106\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average: ', np.mean(bert_r1))\n",
    "print('rouge2 average: ', np.mean(bert_r2))\n",
    "print('rougeL average: ', np.mean(bert_rL))\n",
    "print('rougeLs average:', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7408e182-dd09-4de7-8d0d-b7a349fe156d",
   "metadata": {},
   "source": [
    "**Category 4: entertainment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "53d9ba2f-bf45-444d-b325-a9dcac8a979d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5668</th>\n",
       "      <td>127955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7275</th>\n",
       "      <td>163834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4655</th>\n",
       "      <td>106634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12025</th>\n",
       "      <td>266862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>58192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index\n",
       "5668   127955\n",
       "7275   163834\n",
       "4655   106634\n",
       "12025  266862\n",
       "2458    58192"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_inx = find_indices(categories, 'entertainment')\n",
    "index = pd.DataFrame({\"index\": cat_inx})\n",
    "sample_index = index.sample(n=1000, replace=False, random_state=1004)\n",
    "sample_index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e42bb9e3-017a-481e-819c-c4a62ac80e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id = []\n",
    "url = []\n",
    "title = []\n",
    "article = []\n",
    "summary = []\n",
    "\n",
    "for i in sample_index[\"index\"]:\n",
    "    id.append(dataset[\"train\"][i]['id'])\n",
    "    url.append(dataset[\"train\"][i]['url'])\n",
    "    title.append(dataset[\"train\"][i]['title'])\n",
    "    summary.append(dataset[\"train\"][i]['summary'])\n",
    "    article.append(dataset[\"train\"][i]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a4dedd09-a0d1-443a-a326-6a0d4c974a70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>entertainment-arts-19783861</td>\n",
       "      <td>https://www.bbc.com/news/entertainment-arts-19...</td>\n",
       "      <td>National Theatre reports record annual takings...</td>\n",
       "      <td>By Helen BushbyBBC Entertainment and Arts repo...</td>\n",
       "      <td>The National Theatre has reported record yearl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entertainment-arts-38290366</td>\n",
       "      <td>https://www.bbc.com/news/entertainment-arts-38...</td>\n",
       "      <td>Golden Globes: La La Land leads nominations</td>\n",
       "      <td>Damien Chazelle's movie is up for best musical...</td>\n",
       "      <td>Musical film La La Land leads the pack in this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entertainment-arts-28228750</td>\n",
       "      <td>https://www.bbc.com/news/entertainment-arts-28...</td>\n",
       "      <td>Yorkshire Sculpture Park wins museum of the ye...</td>\n",
       "      <td>By Tim MastersArts and entertainment correspon...</td>\n",
       "      <td>The open-air Yorkshire Sculpture Park has been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entertainment-arts-36979491</td>\n",
       "      <td>https://www.bbc.com/news/entertainment-arts-36...</td>\n",
       "      <td>New Alf Garnett seen in Till Death Us Do Part ...</td>\n",
       "      <td>Simon Day, who has previously appeared in The ...</td>\n",
       "      <td>Alf Garnett has lost his signature moustache b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment-arts-28834671</td>\n",
       "      <td>https://www.bbc.com/news/entertainment-arts-28...</td>\n",
       "      <td>BSkyB makes 20% ethnic minorities pledge</td>\n",
       "      <td>The target applies to all home-grown programme...</td>\n",
       "      <td>The broadcaster, Sky, has pledged that 20% of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id  \\\n",
       "0  entertainment-arts-19783861   \n",
       "1  entertainment-arts-38290366   \n",
       "2  entertainment-arts-28228750   \n",
       "3  entertainment-arts-36979491   \n",
       "4  entertainment-arts-28834671   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.bbc.com/news/entertainment-arts-19...   \n",
       "1  https://www.bbc.com/news/entertainment-arts-38...   \n",
       "2  https://www.bbc.com/news/entertainment-arts-28...   \n",
       "3  https://www.bbc.com/news/entertainment-arts-36...   \n",
       "4  https://www.bbc.com/news/entertainment-arts-28...   \n",
       "\n",
       "                                               title  \\\n",
       "0  National Theatre reports record annual takings...   \n",
       "1        Golden Globes: La La Land leads nominations   \n",
       "2  Yorkshire Sculpture Park wins museum of the ye...   \n",
       "3  New Alf Garnett seen in Till Death Us Do Part ...   \n",
       "4           BSkyB makes 20% ethnic minorities pledge   \n",
       "\n",
       "                                             article  \\\n",
       "0  By Helen BushbyBBC Entertainment and Arts repo...   \n",
       "1  Damien Chazelle's movie is up for best musical...   \n",
       "2  By Tim MastersArts and entertainment correspon...   \n",
       "3  Simon Day, who has previously appeared in The ...   \n",
       "4  The target applies to all home-grown programme...   \n",
       "\n",
       "                                             summary  \n",
       "0  The National Theatre has reported record yearl...  \n",
       "1  Musical film La La Land leads the pack in this...  \n",
       "2  The open-air Yorkshire Sculpture Park has been...  \n",
       "3  Alf Garnett has lost his signature moustache b...  \n",
       "4  The broadcaster, Sky, has pledged that 20% of ...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'id': id, 'url': url, \"title\": title, 'article': article, 'summary': summary}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c569a8a3-f050-4e8d-bf0a-1fe3f570077a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(int(len(df['article']))):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = 2, \n",
    "                      min_length = 45,\n",
    "                      max_length = 497,           \n",
    "                      ratio = None,\n",
    "                      use_first = None,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_7_scores_entertainment.csv', index=False)\n",
    "        print(i)\n",
    "\n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_7_scores_entertainment.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e690109b-3068-4607-92de-a3af82689194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average:  0.1766747367659147\n",
      "rouge2 average:  0.02413272039425646\n",
      "rougeL average:  0.12182373566311334\n",
      "rougeLs average: 0.12182373566311334\n",
      "chrf average: 24.87528790810518\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average: ', np.mean(bert_r1))\n",
    "print('rouge2 average: ', np.mean(bert_r2))\n",
    "print('rougeL average: ', np.mean(bert_rL))\n",
    "print('rougeLs average:', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9728c1f3-c8c7-4a86-97cd-65c9ad25bbc9",
   "metadata": {},
   "source": [
    "**Category 5: technology**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "358d05ca-9d4f-451c-9203-69486e68fc21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7384</th>\n",
       "      <td>245892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>121535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7057</th>\n",
       "      <td>235727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4533</th>\n",
       "      <td>155054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>10474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index\n",
       "7384  245892\n",
       "3450  121535\n",
       "7057  235727\n",
       "4533  155054\n",
       "212    10474"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_inx = find_indices(categories, 'technology')\n",
    "index = pd.DataFrame({\"index\": cat_inx})\n",
    "sample_index = index.sample(n=1000, replace=False, random_state=1004)\n",
    "sample_index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d38b0ff-bc23-4bbc-b359-406f9615036e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id = []\n",
    "url = []\n",
    "title = []\n",
    "article = []\n",
    "summary = []\n",
    "\n",
    "for i in sample_index[\"index\"]:\n",
    "    id.append(dataset[\"train\"][i]['id'])\n",
    "    url.append(dataset[\"train\"][i]['url'])\n",
    "    title.append(dataset[\"train\"][i]['title'])\n",
    "    summary.append(dataset[\"train\"][i]['summary'])\n",
    "    article.append(dataset[\"train\"][i]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "88911c48-3f86-43b7-b620-fd9d3e6bec73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>technology-51493935</td>\n",
       "      <td>https://www.bbc.com/news/technology-51493935</td>\n",
       "      <td>Consumer contract changes 'could save customer...</td>\n",
       "      <td>UK watchdog Ofcom says users could save £150 a...</td>\n",
       "      <td>Broadband, TV and phone customers will be give...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>technology-35077448</td>\n",
       "      <td>https://www.bbc.com/news/technology-35077448</td>\n",
       "      <td>AirBnB racism claim: African-Americans 'less l...</td>\n",
       "      <td>Dave LeeNorth America technology reporter A su...</td>\n",
       "      <td>People with names that suggest they are black ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technology-31753934</td>\n",
       "      <td>https://www.bbc.com/news/technology-31753934</td>\n",
       "      <td>Dozens arrested in cybercrime 'strike week'</td>\n",
       "      <td>In total, 25 separate operations were carried ...</td>\n",
       "      <td>The UK's National Crime Agency has arrested 56...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>technology-23754311</td>\n",
       "      <td>https://www.bbc.com/news/technology-23754311</td>\n",
       "      <td>Minecraft maker shelves 0x10c video game</td>\n",
       "      <td>The new project was provisionally titled 0x10c...</td>\n",
       "      <td>Developer Markus \"Notch\" Persson, who created ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>technology-45083644</td>\n",
       "      <td>https://www.bbc.com/news/technology-45083644</td>\n",
       "      <td>Homes lost after mortgage computer bug</td>\n",
       "      <td>The government scheme was designed to help peo...</td>\n",
       "      <td>A flaw in a US bank's computer software led to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                           url  \\\n",
       "0  technology-51493935  https://www.bbc.com/news/technology-51493935   \n",
       "1  technology-35077448  https://www.bbc.com/news/technology-35077448   \n",
       "2  technology-31753934  https://www.bbc.com/news/technology-31753934   \n",
       "3  technology-23754311  https://www.bbc.com/news/technology-23754311   \n",
       "4  technology-45083644  https://www.bbc.com/news/technology-45083644   \n",
       "\n",
       "                                               title  \\\n",
       "0  Consumer contract changes 'could save customer...   \n",
       "1  AirBnB racism claim: African-Americans 'less l...   \n",
       "2        Dozens arrested in cybercrime 'strike week'   \n",
       "3           Minecraft maker shelves 0x10c video game   \n",
       "4             Homes lost after mortgage computer bug   \n",
       "\n",
       "                                             article  \\\n",
       "0  UK watchdog Ofcom says users could save £150 a...   \n",
       "1  Dave LeeNorth America technology reporter A su...   \n",
       "2  In total, 25 separate operations were carried ...   \n",
       "3  The new project was provisionally titled 0x10c...   \n",
       "4  The government scheme was designed to help peo...   \n",
       "\n",
       "                                             summary  \n",
       "0  Broadband, TV and phone customers will be give...  \n",
       "1  People with names that suggest they are black ...  \n",
       "2  The UK's National Crime Agency has arrested 56...  \n",
       "3  Developer Markus \"Notch\" Persson, who created ...  \n",
       "4  A flaw in a US bank's computer software led to...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'id': id, 'url': url, \"title\": title, 'article': article, 'summary': summary}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8d48b76-35fd-41a3-ad6b-2fb5f516321f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bert_r1 = []\n",
    "bert_r2 = []\n",
    "bert_rL = []\n",
    "bert_rLs = []\n",
    "bert_chrf = []\n",
    "\n",
    "for i in range(int(len(df['article']))):\n",
    "    \n",
    "    # Limiting number sentences in each summary generated to 2 sentences \n",
    "    candidate = model(df['article'][i], \n",
    "                      num_sentences = 2, \n",
    "                      min_length = 45,\n",
    "                      max_length = 497,          \n",
    "                      ratio = None,\n",
    "                      use_first = None,\n",
    "                     )\n",
    "    candidate = [candidate]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    results2 = chrf.compute(predictions=candidate,\n",
    "                            references= ref)\n",
    "    \n",
    "    bert_chrf.append(results2['score'])\n",
    "    \n",
    "    bert_r1.append(results['rouge1'])\n",
    "    bert_r2.append(results['rouge2'])\n",
    "    bert_rL.append(results['rougeL'])\n",
    "    bert_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    if i in np.arange(0, (len(df['article']) + 101), 100):\n",
    "        data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BERT_7_scores_tech.csv', index=False)\n",
    "        print(i)\n",
    "\n",
    "data = {'rouge1': bert_r1, 'rouge2': bert_r2, 'rogueL': bert_rL, 'rogueLs': bert_rLs, 'chrf': bert_chrf}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BERT_7_scores_tech.csv', index=False)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bf940ada-ad3b-41ba-a68e-df45eef6b897",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 average:  0.16956305823103446\n",
      "rouge2 average:  0.020127432450957147\n",
      "rougeL average:  0.11438383130539752\n",
      "rougeLs average: 0.11438383130539752\n",
      "chrf average: 26.085576408431866\n"
     ]
    }
   ],
   "source": [
    "print('rouge1 average: ', np.mean(bert_r1))\n",
    "print('rouge2 average: ', np.mean(bert_r2))\n",
    "print('rougeL average: ', np.mean(bert_rL))\n",
    "print('rougeLs average:', np.mean(bert_rLs))\n",
    "print('chrf average:', np.mean(bert_chrf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
