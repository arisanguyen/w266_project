{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b049999-077d-4d48-a02d-94c842050121",
   "metadata": {
    "tags": []
   },
   "source": [
    "**BART Trained on XL SUM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d1de3e-85a6-4307-8abf-fbfcae0be263",
   "metadata": {},
   "source": [
    "**Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0dfe809d-a7a8-47b6-a2e0-2d7bf7d74fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "import inspect\n",
    "\n",
    "#let's make longer output readable without horizontal scrolling\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "\n",
    "import regex as re\n",
    "\n",
    "import os, re\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# These auto classes load the right type of tokenizer and model based on a model name\n",
    "from transformers import AutoTokenizer, TFAutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ef6ab7-aa59-479f-a0e8-e0e34a21dc7f",
   "metadata": {},
   "source": [
    "**Necessary Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce3330d3-069e-48dc-8a0b-e2ef7d72a2bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "216551a6-cb1e-4516-aa9b-b18d88807a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate.load(\"chrf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a32aaf5-c5bb-4052-8b50-af66dbb96312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_args(func):\n",
    "    signature = inspect.signature(func)\n",
    "    return {\n",
    "        k: v.default\n",
    "        for k, v in signature.parameters.items()\n",
    "        if v.default is not inspect.Parameter.empty\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a4ff58f-fbd5-44ef-94ce-a5463542821a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_organize(sample_index):\n",
    "\n",
    "    article = []\n",
    "    summary = []\n",
    "\n",
    "    for i in sample_index[\"index\"]:\n",
    "        summary.append(dataset[\"train\"][i]['summary'])\n",
    "        article.append(dataset[\"train\"][i]['text'])\n",
    "\n",
    "    return article, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c377e90f-6fa0-444f-bdb1-fa6ef660476c",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2c668da-18f8-43d2-8a1e-c5e9c604f472",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xlsum (/home/ubuntu/.cache/huggingface/datasets/csebuetnlp___xlsum/english/2.0.0/518ab0af76048660bcc2240ca6e8692a977c80e384ffb18fdddebaca6daebdce)\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 330.22it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"csebuetnlp/xlsum\", \"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03beb832-2912-41f6-8014-6d818e2d4418",
   "metadata": {},
   "source": [
    "**Sampling the Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a546358b-d1a6-408c-a79d-a6d24a6b3d6f",
   "metadata": {},
   "source": [
    "**Train, Val, and Test sets for all XL Sum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c948ad56-4a93-4782-bbc1-02aa61b6dd66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235420</th>\n",
       "      <td>235420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172024</th>\n",
       "      <td>172024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253546</th>\n",
       "      <td>253546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224954</th>\n",
       "      <td>224954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214134</th>\n",
       "      <td>214134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index\n",
       "235420  235420\n",
       "172024  172024\n",
       "253546  253546\n",
       "224954  224954\n",
       "214134  214134"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pd.DataFrame({\"index\": list(range(len(dataset['train'])))})\n",
    "sample_index = index.sample(n=2000, replace=False, random_state=1004)\n",
    "sample_index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34e9876f-8055-4817-a6c2-1b1441201534",
   "metadata": {},
   "outputs": [],
   "source": [
    "article, summary = data_organize(sample_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2dfa92fd-ab9c-4ebd-b781-7d9b658414d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'text': article[:1000],  'summary': summary[:1000]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.to_csv('w266_project/xl_sum_sample_train.csv', index = False)\n",
    "#df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91fb59c5-78e4-46c6-8861-e89e7230d633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anthony ZurcherNorth America reporter@awzurche...</td>\n",
       "      <td>On day three of public hearings in the impeach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It made a net profit of $281m (£185m) in the t...</td>\n",
       "      <td>Yum Brands, owner of KFC and Pizza Hut restaur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Police sources told local media that the boy h...</td>\n",
       "      <td>Four members of the same family have been arre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zelda Perkins told the Financial Times she sig...</td>\n",
       "      <td>A British former assistant of Harvey Weinstein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bus workers walked out on Monday over changes ...</td>\n",
       "      <td>Bus drivers in Jersey have agreed to meet with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Anthony ZurcherNorth America reporter@awzurche...   \n",
       "1  It made a net profit of $281m (£185m) in the t...   \n",
       "2  Police sources told local media that the boy h...   \n",
       "3  Zelda Perkins told the Financial Times she sig...   \n",
       "4  Bus workers walked out on Monday over changes ...   \n",
       "\n",
       "                                             summary  \n",
       "0  On day three of public hearings in the impeach...  \n",
       "1  Yum Brands, owner of KFC and Pizza Hut restaur...  \n",
       "2  Four members of the same family have been arre...  \n",
       "3  A British former assistant of Harvey Weinstein...  \n",
       "4  Bus drivers in Jersey have agreed to meet with...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'text': article[1000:1100],  'summary': summary[1000:1100]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.to_csv('w266_project/xl_sum_sample_val.csv', index = False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d0c3b62-f356-4f57-928f-c3f07b461c12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In a statement, the White House said more than...</td>\n",
       "      <td>Hackers who breached US government networks st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fourteen fire engines were sent to tackle the ...</td>\n",
       "      <td>A \"major\" fire on Bognor Regis seafront has be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The 240-turbine Atlantic Array would be double...</td>\n",
       "      <td>The European Commission (EC) has launched an i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is believed the patient contracted the infe...</td>\n",
       "      <td>A patient has been diagnosed with the rare vir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By Chris JohnstonBusiness reporter The rise in...</td>\n",
       "      <td>UK inflation will quadruple to about 4% in the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  In a statement, the White House said more than...   \n",
       "1  Fourteen fire engines were sent to tackle the ...   \n",
       "2  The 240-turbine Atlantic Array would be double...   \n",
       "3  It is believed the patient contracted the infe...   \n",
       "4  By Chris JohnstonBusiness reporter The rise in...   \n",
       "\n",
       "                                             summary  \n",
       "0  Hackers who breached US government networks st...  \n",
       "1  A \"major\" fire on Bognor Regis seafront has be...  \n",
       "2  The European Commission (EC) has launched an i...  \n",
       "3  A patient has been diagnosed with the rare vir...  \n",
       "4  UK inflation will quadruple to about 4% in the...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'text': article[1100:1200],  'summary': summary[1100:1200]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.to_csv('w266_project/xl_sum_sample_test.csv', index = False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5ee779-4ba6-44bf-ad0b-7ee163c6a44e",
   "metadata": {},
   "source": [
    "**Train, Val, and Test sets for each category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a3c4e98-f79e-45e3-9c79-40c8f5169b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_indices(list_to_check, item_to_find):\n",
    "    indices = []\n",
    "    for idx, value in enumerate(list_to_check):\n",
    "        if value == item_to_find:\n",
    "            indices.append(idx)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d626de4-e65a-44ad-9b23-ba0c69cd6a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories = []\n",
    "\n",
    "for i in range(len(dataset['train'])):\n",
    "    cat = dataset['train'][i]['id']\n",
    "    result = re.sub('\\d','',cat)[:-1]\n",
    "    result = result.split('-')[0].split('.')[0]\n",
    "    categories.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee534d9-f692-490d-b4b3-5d94c2b7b5ec",
   "metadata": {},
   "source": [
    "**Category 1: uk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520b09b1-2d30-4269-b0ad-658a0e88f440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uk = find_indices(categories, 'uk')\n",
    "index = pd.DataFrame({\"index\": uk})\n",
    "sample_index = index.sample(n=2000, replace=False, random_state=1004)\n",
    "\n",
    "article, summary = data_organize(sample_index)\n",
    "\n",
    "d = {'text': article[:1000],  'summary': summary[:1000]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.to_csv('xl_sum_sample_train_uk')\n",
    "df.head(5)\n",
    "\n",
    "d = {'text': article[1000:1100],  'summary': summary[1000:1100]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.to_csv('xl_sum_sample_val_uk')\n",
    "df.head(5)\n",
    "\n",
    "d = {'text': article[1100:1200],  'summary': summary[1100:1200]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.to_csv('xl_sum_sample_test_uk')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44de8cc2-471c-4ad3-b94d-8473c6c3c244",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Training the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b616e-4e2b-46eb-83b7-75930d6a43ee",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6399dc-0acd-49fc-a5f7-4c7d4e689781",
   "metadata": {},
   "source": [
    "**All Categories**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdae58a-e4c0-4a6c-8f67-b8309a24c665",
   "metadata": {},
   "source": [
    "All Categories: Model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41489617-caa3-421b-861a-7716cb853985",
   "metadata": {},
   "outputs": [],
   "source": [
    "    --push_to_hub_model_id finetuned-BART-all-categories \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd44f23f-89ae-4365-9842-202417895445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/w266/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "04/02/2023 09:25:10 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "04/02/2023 09:25:10 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=w266_project/finetuned-BART-all-categories/runs/Apr02_09-25-09_ip-172-31-55-240,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=w266_project/finetuned-BART-all-categories,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=w266_project/finetuned-BART-all-categories,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "04/02/2023 09:25:10 - INFO - datasets.builder - Using custom data configuration default-19a9785f6d12354e\n",
      "04/02/2023 09:25:10 - INFO - datasets.info - Loading Dataset Infos from /home/ubuntu/w266/lib/python3.8/site-packages/datasets/packaged_modules/csv\n",
      "04/02/2023 09:25:10 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "04/02/2023 09:25:10 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/csv/default-19a9785f6d12354e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
      "04/02/2023 09:25:10 - WARNING - datasets.builder - Found cached dataset csv (/home/ubuntu/.cache/huggingface/datasets/csv/default-19a9785f6d12354e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "04/02/2023 09:25:10 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/csv/default-19a9785f6d12354e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 90.73it/s]\n",
      "[INFO|configuration_utils.py:668] 2023-04-02 09:25:10,117 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-04-02 09:25:10,127 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:494] 2023-04-02 09:25:10,157 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:668] 2023-04-02 09:25:10,183 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-04-02 09:25:10,184 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-02 09:25:10,241 >> loading file vocab.json from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-02 09:25:10,241 >> loading file merges.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-02 09:25:10,241 >> loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-02 09:25:10,241 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-02 09:25:10,241 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-04-02 09:25:10,241 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:668] 2023-04-02 09:25:10,241 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-04-02 09:25:10,242 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2451] 2023-04-02 09:25:10,375 >> loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:575] 2023-04-02 09:25:15,477 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.28.0.dev0\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3100] 2023-04-02 09:25:17,937 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3108] 2023-04-02 09:25:17,937 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "[INFO|modeling_utils.py:2753] 2023-04-02 09:25:17,972 >> Generation config file not found, using a generation config created from the model config.\n",
      "04/02/2023 09:25:18 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/csv/default-19a9785f6d12354e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-298457322c751ce6.arrow\n",
      "04/02/2023 09:25:18 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/csv/default-19a9785f6d12354e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f9e523c92b03ad4b.arrow\n",
      "/home/ubuntu/w266_project/finetuned-BART-all-categories is already a clone of https://huggingface.co/arisanguyen/finetuned-BART-all-categories. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "04/02/2023 09:25:18 - WARNING - huggingface_hub.repository - /home/ubuntu/w266_project/finetuned-BART-all-categories is already a clone of https://huggingface.co/arisanguyen/finetuned-BART-all-categories. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "/home/ubuntu/w266/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1746] 2023-04-02 09:25:21,036 >> ***** Running training *****\n",
      "[INFO|trainer.py:1747] 2023-04-02 09:25:21,036 >>   Num examples = 1000\n",
      "[INFO|trainer.py:1748] 2023-04-02 09:25:21,036 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1749] 2023-04-02 09:25:21,036 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1750] 2023-04-02 09:25:21,036 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1751] 2023-04-02 09:25:21,036 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1752] 2023-04-02 09:25:21,036 >>   Total optimization steps = 96\n",
      "[INFO|trainer.py:1753] 2023-04-02 09:25:21,037 >>   Number of trainable parameters = 139420416\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-04-02 09:25:21,122 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "! python3 transformers/examples/pytorch/summarization/run_summarization.py \\\n",
    "    --model_name_or_path=facebook/bart-base \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_file='w266_project/xl_sum_sample_train.csv' \\\n",
    "    --validation_file='w266_project/xl_sum_sample_val.csv' \\\n",
    "    --push_to_hub \\\n",
    "    --output_dir='w266_project/finetuned-BART-all-categories' \\\n",
    "    --overwrite_output_dir \\\n",
    "    --per_device_train_batch_size=32 \\\n",
    "    --per_device_eval_batch_size=32 \\\n",
    "    --predict_with_generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c0f435-0c5b-4e80-b398-67bee54174d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    --hub_model_id='arisanguyen/finetuned-BART-all-categories' \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0e2b3649-44fe-4d54-98fd-7041c85b2c21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
     ]
    }
   ],
   "source": [
    "! repo.git_pull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c26b8a9c-6385-4f29-b3fc-caa2bbbd1c10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "arisanguyen/finetuned-BART-all-categories does not appear to have a file named config.json. Checkout 'https://huggingface.co/arisanguyen/finetuned-BART-all-categories/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/w266/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:259\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/w266/lib/python3.8/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/arisanguyen/finetuned-BART-all-categories/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/w266/lib/python3.8/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m~/w266/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/w266/lib/python3.8/site-packages/huggingface_hub/file_download.py:1134\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/w266/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/w266/lib/python3.8/site-packages/huggingface_hub/file_download.py:1475\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1466\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1467\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1468\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1473\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1474\u001b[0m )\n\u001b[0;32m-> 1475\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/w266/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:269\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    268\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EntryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-642948db-2a9ffe3338381e225df2c3e0)\n\nEntry Not Found for url: https://huggingface.co/arisanguyen/finetuned-BART-all-categories/resolve/main/config.json.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marisanguyen/finetuned-BART-all-categories\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m bart_r1 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m bart_r2 \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/w266/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:441\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs_copy\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    439\u001b[0m         _ \u001b[38;5;241m=\u001b[39m kwargs_copy\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 441\u001b[0m     config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/w266/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:896\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;129m@replace_list_option_in_docstrings\u001b[39m()\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    823\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;124;03m    Instantiate one of the configuration classes of the library from a pretrained model configuration.\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \n\u001b[1;32m    826\u001b[0m \u001b[38;5;124;03m    The configuration class to instantiate is selected based on the `model_type` property of the config object that\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;124;03m    is loaded, or when it's missing, by falling back to using pattern matching on `pretrained_model_name_or_path`:\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \n\u001b[1;32m    829\u001b[0m \u001b[38;5;124;03m    List options\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \n\u001b[1;32m    831\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;124;03m        pretrained_model_name_or_path (`str` or `os.PathLike`):\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;124;03m            Can be either:\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \n\u001b[1;32m    835\u001b[0m \u001b[38;5;124;03m                - A string, the *model id* of a pretrained model configuration hosted inside a model repo on\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;124;03m                  huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`, or\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;124;03m                  namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;124;03m                - A path to a *directory* containing a configuration file saved using the\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;124;03m                  [`~PretrainedConfig.save_pretrained`] method, or the [`~PreTrainedModel.save_pretrained`] method,\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;124;03m                  e.g., `./my_model_directory/`.\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;124;03m                - A path or url to a saved configuration JSON *file*, e.g.,\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;124;03m                  `./my_model_directory/configuration.json`.\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;124;03m        cache_dir (`str` or `os.PathLike`, *optional*):\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;124;03m            Path to a directory in which a downloaded pretrained model configuration should be cached if the\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;124;03m            standard cache should not be used.\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;124;03m        force_download (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;124;03m            Whether or not to force the (re-)download the model weights and configuration files and override the\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;124;03m            cached versions if they exist.\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;124;03m        resume_download (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03m            Whether or not to delete incompletely received files. Will attempt to resume the download if such a\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;124;03m            file exists.\u001b[39;00m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;124;03m        proxies (`Dict[str, str]`, *optional*):\u001b[39;00m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;124;03m            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;124;03m            'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;124;03m        revision (`str`, *optional*, defaults to `\"main\"`):\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;124;03m            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;124;03m            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;124;03m            identifier allowed by git.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;124;03m        return_unused_kwargs (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m            If `False`, then this function returns just the final configuration object.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03m            If `True`, then this functions returns a `Tuple(config, unused_kwargs)` where *unused_kwargs* is a\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;124;03m            dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;124;03m            part of `kwargs` which has not been used to update `config` and is otherwise ignored.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;124;03m        trust_remote_code (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;124;03m            Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;124;03m            should only be set to `True` for repositories you trust and in which you have read the code, as it will\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;124;03m            execute code present on the Hub on your local machine.\u001b[39;00m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;124;03m        kwargs(additional keyword arguments, *optional*):\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;124;03m            The values in kwargs of any keys which are configuration attributes will be used to override the loaded\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;124;03m            values. Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;124;03m            by the `return_unused_kwargs` keyword parameter.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m \n\u001b[1;32m    874\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \n\u001b[1;32m    876\u001b[0m \u001b[38;5;124;03m    ```python\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;124;03m    >>> from transformers import AutoConfig\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \n\u001b[1;32m    879\u001b[0m \u001b[38;5;124;03m    >>> # Download configuration from huggingface.co and cache.\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;124;03m    >>> config = AutoConfig.from_pretrained(\"bert-base-uncased\")\u001b[39;00m\n\u001b[1;32m    881\u001b[0m \n\u001b[1;32m    882\u001b[0m \u001b[38;5;124;03m    >>> # Download configuration from huggingface.co (user-uploaded) and cache.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;124;03m    >>> config = AutoConfig.from_pretrained(\"dbmdz/bert-base-german-cased\")\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \n\u001b[1;32m    885\u001b[0m \u001b[38;5;124;03m    >>> # If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m    >>> config = AutoConfig.from_pretrained(\"./test/bert_saved_model/\")\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    >>> # Load a specific configuration file.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;124;03m    >>> config = AutoConfig.from_pretrained(\"./test/bert_saved_model/my_configuration.json\")\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03m    >>> # Change some config attributes when loading a pretrained config.\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;124;03m    >>> config = AutoConfig.from_pretrained(\"bert-base-uncased\", output_attentions=True, foo=False)\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;124;03m    >>> config.output_attentions\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \n\u001b[0;32m--> 896\u001b[0m \u001b[38;5;124;03m    >>> config, unused_kwargs = AutoConfig.from_pretrained(\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    ...     \"bert-base-uncased\", output_attentions=True, foo=False, return_unused_kwargs=True\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;124;03m    ... )\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;124;03m    >>> config.output_attentions\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \n\u001b[1;32m    902\u001b[0m \u001b[38;5;124;03m    >>> unused_kwargs\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03m    {'foo': False}\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m    905\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_auto\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    906\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n",
      "File \u001b[0;32m~/w266/lib/python3.8/site-packages/transformers/configuration_utils.py:573\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 573\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    575\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/w266/lib/python3.8/site-packages/transformers/configuration_utils.py:628\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/w266/lib/python3.8/site-packages/transformers/utils/hub.py:454\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m         revision \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    457\u001b[0m     )\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# First we try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m try_to_load_from_cache(path_or_repo_id, full_filename, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir, revision\u001b[38;5;241m=\u001b[39mrevision)\n",
      "\u001b[0;31mOSError\u001b[0m: arisanguyen/finetuned-BART-all-categories does not appear to have a file named config.json. Checkout 'https://huggingface.co/arisanguyen/finetuned-BART-all-categories/main' for available files."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"arisanguyen/finetuned-BART-all-categories\"\n",
    ")\n",
    "\n",
    "bart_r1 = []\n",
    "bart_r2 = []\n",
    "bart_rL = []\n",
    "bart_rLs = []\n",
    "bart_chrf = []\n",
    "\n",
    "for i in range(int(len(df['article'])/2)):\n",
    "    \n",
    "    #art = ' '.join(df['article'][i].split(' ')[:1024]) #truncated to first 1024 words, because that is all the model can handle\n",
    "    \n",
    "    candidate = summarizer(df['article'][i], \n",
    "                           truncation = True, #truncated to first 1024 words, because that is all the model can handle\n",
    "                             #max_length=130, min_length=30, do_sample=False\n",
    "                            )[0]\n",
    "    candidate = [candidate['summary_text']]\n",
    "    #pprint(candidate[0], compact=True)\n",
    "    \n",
    "    ref = [df['summary'][i]]\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bart_r1.append(results['rouge1'])\n",
    "    bart_r2.append(results['rouge2'])\n",
    "    bart_rL.append(results['rougeL'])\n",
    "    bart_rLs.append(results['rougeLsum'])\n",
    "    \n",
    "    results = rouge.compute(predictions=candidate,\n",
    "                            references=ref)\n",
    "    \n",
    "    bart_chrf.append(results['score'])\n",
    "    \n",
    "    if i in np.arange(0, 2000, 100):\n",
    "        data = {'rouge1': bart_r1, 'rouge2': bart_r2, 'rogueL': bart_rL, 'rogueLs': bart_rLs, 'chrf': bart_chrf}\n",
    "        scores = pd.DataFrame(data)\n",
    "        scores.to_csv(r'BART_trained_0_scores.csv', index=False)\n",
    "        print(i)\n",
    "        \n",
    "data = {'rouge1': bart_r1, 'rouge2': bart_r2, 'rogueL': bart_rL, 'rogueLs': bart_rLs,'chrf': bart_chrf}\n",
    "scores = pd.DataFrame(data)\n",
    "scores.to_csv(r'BART_trained_0_scores.csv', index=False)\n",
    "\n",
    "print('Last Article', df['article'][i])\n",
    "print('Last Reference Summary', ref)\n",
    "print('Last Candidate Summary', candidate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
